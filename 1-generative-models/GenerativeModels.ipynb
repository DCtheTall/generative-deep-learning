{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenerativeModels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyeXGqQqy3DQ",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 1: Generative Models\n",
        "\n",
        "## Generative Versus Discriminative Modeling\n",
        "\n",
        "- A _discriminative model_ estimates $p(y|\\mathbf{x})$, i.e. the probability of a label $y$ given an observation $\\mathbf{x}$.\n",
        "\n",
        "- A _generative model_ estimates $p(\\mathbf{x})$, i.e. the probability of observing observation $\\mathbf{x}$. If the dataset is labeled, then a generative model estimates the distribution $p(y|\\mathbf{x})$.\n",
        "\n",
        "## The Generative Modeling Framework\n",
        "\n",
        "- We have a dataset of observations $\\mathbf{X}$.\n",
        "\n",
        "- We assume the observations have been generated according to some unknown distribution, $p_\\text{data}$.\n",
        "\n",
        "- A generative model $p_\\text{model}$ tries to mimic $p_\\text{data}$ so that we can generate new observations.\n",
        "\n",
        "We are generally impressed with $p_\\text{model}$ if:\n",
        "\n",
        "1. It can generate observations that appear to have been drawn from $p_\\text{data}$.\n",
        "\n",
        "2. It can generate examples suitably different from the observations it has seen, i.e. it does not just copy previously seen observations.\n",
        "\n",
        "## Probabilistic Generative Models\n",
        "\n",
        "### Sample Space\n",
        "\n",
        "A _sample space_ is the complete set of all values that $\\mathbf{x}$ can take.\n",
        "\n",
        "### Probability Density Function\n",
        "\n",
        "A _probability density function_ (or _density function_), $p(\\mathbf{x})$, is a function from the sample space, $S$, to the interval [0, 1] such that\n",
        "\n",
        "$$ \\int_{\\mathbf{x}\\,\\in\\,S} p(\\mathbf{x})\\,d\\mathbf{x} = 1 $$\n",
        "\n",
        "if $S$ is continuous or\n",
        "\n",
        "$$ \\sum\\limits_{\\mathbf{x}\\,\\in\\,S} p(\\mathbf{x}) = 1 $$\n",
        "\n",
        "if $S$ is discrete.\n",
        "\n",
        "### Parametric Modeling\n",
        "\n",
        "A _parametric model_, $p_\\theta(\\mathbf{x})$, is a family of density functions that can be described using a finite number of parameters represented by the vector, $\\theta$.\n",
        "\n",
        "### Likelihood\n",
        "\n",
        "The _likelihood_, $\\mathcal{L} (\\theta | \\mathbf{x})$, of a parameter set $\\theta$ is a function that gives you the likelihood of $\\theta$ given some observed point $\\mathbf{x}$ is defined to be the value of a density function parameterized by theta. It is defined as\n",
        "\n",
        "$$ \\mathcal{L}(\\theta|\\mathbf{x}) = p_\\theta(\\mathbf{x}). $$\n",
        "\n",
        "Given a dataset, $\\mathbf{X}$, the likelihood of the distribution parameterized by $\\theta$ is given by\n",
        "\n",
        "$$ \\mathcal{L}(\\theta|\\mathbf{X}) = \\prod\\limits_{\\mathbf{x}\\,\\in\\,\\mathbf{X}} p_\\theta (\\mathbf{x}). $$\n",
        "\n",
        "It is more common to use the _log-likelihood_ function instead, which is less computationally intense:\n",
        "\n",
        "$$ \\ell(\\theta|\\mathbf{X}) = \\prod\\limits_{\\mathbf{x}\\,\\in\\,\\mathbf{X}} \\log p_\\theta(\\mathbf{x}). $$\n",
        "\n",
        "### Maximum Likelihood Estimation\n",
        "\n",
        "The _maximum likelihood estimation_ (MLE) is defined by the set of parameters, $\\hat{\\theta}$, which maximize the likelihood function, i.e.\n",
        "\n",
        "$$ \\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} \\, \\mathcal{L}(\\theta|\\mathbf{X}). $$\n",
        "\n",
        "## Probabalistic Generative Models\n",
        "\n",
        "### Multinomial Distribution\n",
        "\n",
        "The _multinomial distribution_ assigns each combination of features a probablity given by the number of times that combination appears in the original dataset. The MLE, $\\hat{\\theta}_j$ is given by\n",
        "\n",
        "$$ \\hat{\\theta}_j = \\frac{n_j}{N}. $$\n",
        "\n",
        "The one clear downside of this distribution is that it cannot be used to generate any new samples. This can be remedied is to add a _pseudocount_ of 1 to each possible combination. This would make the MLE\n",
        "\n",
        "$$ \\hat{\\theta}_j = \\frac{n_j + 1}{N + d} $$\n",
        "\n",
        "where $d$ is the number of total possible combinations of features. This technique is known as _additive smoothing_. This technique ensures that all combinations have some probability of being observed, but it also means that any unobserved combination has an equal probability of being drawn.\n",
        "\n",
        "### Naive Bayes\n",
        "\n",
        "The _Naive Bayes_ assumption assumes that for all features of the dataset, $x_j$ and $x_k$, we have that\n",
        "\n",
        "$$ p\\left(x_j | x_k\\right) = p\\left(x_j\\right) $$\n",
        "\n",
        "i.e. that any two features' values are independent of each other. Using this assumption and the chain rule of probability, we can compute the probability of an observation, $\\mathbf{x}$\n",
        "\n",
        "$$ \\begin{align} p(\\mathbf{x}) & = p\\left(x_1,...,x_K\\right) \\\\ & = p\\left(x_2,...,x_K|x_1\\right)p\\left(x_1\\right) \\\\ & = p\\left(x_3,...,x_K|x_1,x_2\\right) p\\left(x_2|x_1\\right) p\\left(x_1\\right) \\\\ & = \\prod\\limits_{k=1}^K p\\left(x_k | x_1,...,x_{k-1}\\right) \\end {align} $$\n",
        "\n",
        "where $K$ is the number of features. Using the Naive Bayes' assumption, we can simplify this expression to\n",
        "\n",
        "$$ p(\\mathbf{x}) = \\prod\\limits_{k=1}^K p\\left(x_k\\right) $$\n",
        "\n",
        "This is the Naive Bayes' model. The problem of finding the best model is to finding the best parameters\n",
        "\n",
        "$$ \\theta_{kl} = p\\left(x_k = l\\right). $$\n",
        "\n",
        "The MLE $\\hat{\\theta}_{kl}$ is given by\n",
        "\n",
        "$$ \\hat{\\theta}_{kl} = \\frac{n_{kl}}{N} $$\n",
        "\n",
        "where $n_{kl}$ is the number of times $x_k$ has the value $l$ in the dataset of observations."
      ]
    }
  ]
}