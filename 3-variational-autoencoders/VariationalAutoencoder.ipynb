{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VariationalAutoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNKdHOQE-RZe",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 3: Variational Autoencoders\n",
        "\n",
        "## Building a Variational Autoencoder\n",
        "\n",
        "A _variational autoencoder_ (VAE) is an autoencoder that encodes inputs into a _normal distribution_ that can be used to sample the latent space. The normal distribution in one dimension is given by\n",
        "\n",
        "$$ f \\left( x \\,|\\, \\mu, \\sigma \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp \\left( - \\frac{(x - \\mu)^2}{2\\sigma^2} \\right) $$\n",
        "\n",
        "Its parameters are the _mean_, $\\mu$, and the standard deviation, $\\sigma$. The _standard normal distribution_ has a mean of zero and a standard deviation of one. The multidimensional normal distribution is given by\n",
        "\n",
        "$$ f \\left( \\mathbf{x} \\,|\\, \\mu,\\sigma \\right) =  \\frac{\\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\mu)^T \\Sigma^{-1} (\\mathbf{x} - \\mu) \\right)}{\\sqrt{(2\\pi)^k|\\Sigma|}} $$\n",
        "\n",
        "where $k$ is the number of dimensions, $\\mu$ is the mean vector, and $\\Sigma$ is the _covariance matrix_, which is given by\n",
        "\n",
        "$$ \\Sigma_{ij} = \\left\\{ \\begin{matrix} \\sigma_i^2 && \\text{if}\\;i = j \\\\ \\rho_{ij} \\sigma_i\\sigma_j && \\text{otherwise} \\end{matrix} \\right. $$\n",
        "\n",
        "where $\\rho_{ij}$ is the correlation between the two dimensions. Variational autoencoders assume that $\\rho = 0$, i.e. that the dimensions in the latent space have no correlation. This means $\\Sigma$ is a diagonal matrix whose values are th variance in each dimension. When training a VAE, we use the logarithm of the variance since that can fall anywhere on the interval $(-\\infty,\\infty)$.\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "In addition to using MSE for the reconstruction loss, we also introduce the _Kullback Leibler (KL) divergence_, a way to measure how different a probability distribution is from another. In general it is given by\n",
        "\n",
        "$$ \\begin{align} D_{KL}\\left(P\\,||\\,Q\\right) & = -\\sum\\limits_{x\\in X} p(x)\\log q(x) + \\sum\\limits_{x\\in X} p(x)\\log p(x) \\\\ & = H(P,Q) - H(P) \\end{align} $$\n",
        "\n",
        "where $H(P,Q)$ is the _cross-entropy_ between $P$ and $Q$, and $H(P)$ is the entropy of P. In our case, we are measuring the KL divergence between the model and the standard normal distribution. In this special case, the KL divergence has the closed form\n",
        "\n",
        "$$ D_{KL}\\left[N(\\mu,\\sigma)\\,||\\,N(0,1)\\right] = \\frac{1}{2}\\sum\\left(1+\\log\\left(\\sigma^2\\right)-\\mu^2-\\sigma^2\\right) $$\n",
        "\n",
        "### Using a VAE to Reconstruct Digits\n",
        "\n",
        "Below is an example implementation of a VAE, we will use this to reconstruct handwritten digits from the MNIST dataset. Later on, we will use it to reconstruct faces from the CelebA dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2x17w8d-QvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google drive to save the weights after training.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7tQoMUj6HG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup, this cell should be run every time.\n",
        "\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization,\n",
        "                                     LeakyReLU, Dropout, Flatten, Dense,\n",
        "                                     Lambda, Reshape, Conv2DTranspose,\n",
        "                                     Activation)\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import KLDivergence, MeanSquaredError\n",
        "\n",
        "\n",
        "def sampling(args):\n",
        "  \"\"\"Sample an encoding from the learned distribution.\"\"\"\n",
        "  mu, log_var = args\n",
        "  return mu + K.exp(log_var / 2) * K.random_normal(shape=K.shape(mu))\n",
        "\n",
        "\n",
        "def step_decay_schedule(initial_lr, decay_factor=0.5, step_size=1):\n",
        "  \"\"\"Create a LearningRateScheduler callback to decay the learning rate during training.\"\"\"\n",
        "  def schedule(epoch):\n",
        "    return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "  return LearningRateScheduler(schedule)\n",
        "\n",
        "\n",
        "class VariationalAutoencoder():\n",
        "  \"\"\"Implements a Varational Autoencoder.\"\"\"\n",
        "\n",
        "  def __init__(self, input_shape, encoder_conv_filters,\n",
        "               encoder_conv_kernel_size, encoder_conv_strides,\n",
        "               decoder_conv_filters, decoder_conv_kernel_size,\n",
        "               decoder_conv_strides, z_dim, use_batch_normalization=False,\n",
        "               use_dropout=False, dropout_rate=0.25):\n",
        "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "    x = encoder_input\n",
        "    for i in range(len(encoder_conv_kernel_size)):\n",
        "      x = Conv2D(filters=encoder_conv_filters[i],\n",
        "                 kernel_size=encoder_conv_kernel_size[i],\n",
        "                 strides=encoder_conv_strides[i], padding='same',\n",
        "                 name='encoder_conv_{}'.format(i + 1))(x)\n",
        "      if use_batch_normalization:\n",
        "        x = BatchNormalization()(x)\n",
        "      x = LeakyReLU()(x)\n",
        "      if use_dropout:\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "    shape_before_flattening = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(z_dim, name='mu')(x)\n",
        "    self.log_var = Dense(z_dim, name='log_var')(x)\n",
        "    self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
        "    encoder_output = Lambda(sampling,\n",
        "                            name='encoder_output')([self.mu, self.log_var])\n",
        "    self.encoder = Model(encoder_input, encoder_output)\n",
        "\n",
        "    decoder_input = Input(shape=(z_dim,), name='decoder_input')\n",
        "    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "    x = Reshape(shape_before_flattening)(x)\n",
        "    for i in range(len(decoder_conv_kernel_size)):\n",
        "      x = Conv2DTranspose(filters=decoder_conv_filters[i],\n",
        "                          kernel_size=decoder_conv_kernel_size[i],\n",
        "                          strides=decoder_conv_strides[i], padding='same',\n",
        "                          name='decoder_conv_t_{}'.format(i + 1))(x)\n",
        "      if i < len(decoder_conv_kernel_size) - 1:\n",
        "        if use_batch_normalization:\n",
        "          x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        if use_dropout:\n",
        "          x = Dropout(rate=dropout_rate)(x)\n",
        "      else:\n",
        "        x = Activation('sigmoid')(x)\n",
        "    decoder_output = x\n",
        "    self.decoder = Model(decoder_input, decoder_output)\n",
        "    self.model = Model(encoder_input, self.decoder(encoder_output))\n",
        "    self.compiled = False\n",
        "    self.learning_rate = None\n",
        "\n",
        "  def compile(self, learning_rate, r_loss_factor):\n",
        "    \"\"\"Compile the model.\"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    if self.compiled:\n",
        "      return\n",
        "    opt = Adam(lr=learning_rate)\n",
        "\n",
        "    mse = MeanSquaredError()\n",
        "    kl_divergence = KLDivergence()\n",
        "\n",
        "    def loss(y_act, y_pred):\n",
        "      return mse(y_act, y_pred) + kl_divergence(y_act, y_pred)\n",
        "    \n",
        "    self.model.compile(opt, loss=loss, metrics=[mse, kl_divergence],\n",
        "                       experimental_run_tf_function=False)\n",
        "    self.compiled = True\n",
        "\n",
        "  def fit(self, X, y, batch_size, epochs, checkpoint_path=None, lr_decay=1):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    if not self.compiled:\n",
        "      raise Exception('Model not compiled')\n",
        "    lr_sched = step_decay_schedule(initial_lr=self.learning_rate,\n",
        "                                   decay_factor=lr_decay, step_size=1)\n",
        "    callbacks = [lr_sched]\n",
        "    if checkpoint_path:\n",
        "      callbacks.append(ModelCheckpoint(filepath=checkpoint_path, verbose=1,\n",
        "                                       save_weights_only=True))\n",
        "    self.model.fit(X, y, batch_size=batch_size, epochs=epochs, shuffle=True,\n",
        "                   callbacks=callbacks)\n",
        "\n",
        "  def load(self, checkpoint_path):\n",
        "    \"\"\"Load the model weights from the saved checkpoint file.\"\"\"\n",
        "    self.model.load_weights(checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K74TLKIT9DMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "3df5d6dd-3376-485b-8dae-d64f475832e4"
      },
      "source": [
        "mnist_vae = VariationalAutoencoder(input_shape=(28, 28, 1),\n",
        "                                   encoder_conv_filters=(32, 64, 64, 64),\n",
        "                                   encoder_conv_kernel_size=(3, 3, 3, 3),\n",
        "                                   encoder_conv_strides=(1, 2, 2, 1),\n",
        "                                   decoder_conv_filters=(64, 64, 32, 1),\n",
        "                                   decoder_conv_kernel_size=(3, 3, 3, 3),\n",
        "                                   decoder_conv_strides=(1, 2, 2, 1),\n",
        "                                   z_dim=2)\n",
        "mnist_vae.model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_1 (Conv2D)         (None, 28, 28, 32)   320         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 28, 28, 32)   0           encoder_conv_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_2 (Conv2D)         (None, 14, 14, 64)   18496       leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 14, 14, 64)   0           encoder_conv_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_3 (Conv2D)         (None, 7, 7, 64)     36928       leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 7, 7, 64)     0           encoder_conv_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_4 (Conv2D)         (None, 7, 7, 64)     36928       leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 7, 7, 64)     0           encoder_conv_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 3136)         0           leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 2)            6274        flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "log_var (Dense)                 (None, 2)            6274        flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 2)            0           mu[0][0]                         \n",
            "                                                                 log_var[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "model_10 (Model)                (None, 28, 28, 1)    102017      encoder_output[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 207,237\n",
            "Trainable params: 207,237\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ__taPrGnfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the MNIST dataset.\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-PsjdeJGqqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape these to be 4D tensors and scale the pixel values to [0, 1].\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape + (1,)) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape + (1,)) / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IBhFrMhHE4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The path to save the model weights to in Drive.\n",
        "\n",
        "checkpoint_path = '/content/gdrive/My Drive/gdl_models/mnist_vae.hdf5'\n",
        "mnist_vae.compile(learning_rate=0.0005, r_loss_factor=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PVzUajyHexn",
        "colab_type": "text"
      },
      "source": [
        "Now we will train the model for 200 training epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if4vzWhPHmI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ca33bbb-e581-4262-f8d2-1421f7a41bc3"
      },
      "source": [
        "mnist_vae.fit(X_train, X_train, batch_size=32, epochs=200,\n",
        "              checkpoint_path=checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0876 - mean_squared_error: 0.0713 - kullback_leibler_divergence: 0.0163\n",
            "Epoch 00001: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0876 - mean_squared_error: 0.0713 - kullback_leibler_divergence: 0.0163 - lr: 5.0000e-04\n",
            "Epoch 2/200\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.0831 - mean_squared_error: 0.0686 - kullback_leibler_divergence: 0.0145\n",
            "Epoch 00002: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0831 - mean_squared_error: 0.0686 - kullback_leibler_divergence: 0.0145 - lr: 5.0000e-04\n",
            "Epoch 3/200\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0807 - mean_squared_error: 0.0673 - kullback_leibler_divergence: 0.0135\n",
            "Epoch 00003: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0807 - mean_squared_error: 0.0673 - kullback_leibler_divergence: 0.0135 - lr: 5.0000e-04\n",
            "Epoch 4/200\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0789 - mean_squared_error: 0.0663 - kullback_leibler_divergence: 0.0127\n",
            "Epoch 00004: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0789 - mean_squared_error: 0.0663 - kullback_leibler_divergence: 0.0127 - lr: 5.0000e-04\n",
            "Epoch 5/200\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0778 - mean_squared_error: 0.0656 - kullback_leibler_divergence: 0.0122\n",
            "Epoch 00005: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0778 - mean_squared_error: 0.0656 - kullback_leibler_divergence: 0.0122 - lr: 5.0000e-04\n",
            "Epoch 6/200\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0767 - mean_squared_error: 0.0650 - kullback_leibler_divergence: 0.0117\n",
            "Epoch 00006: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0767 - mean_squared_error: 0.0649 - kullback_leibler_divergence: 0.0117 - lr: 5.0000e-04\n",
            "Epoch 7/200\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0758 - mean_squared_error: 0.0645 - kullback_leibler_divergence: 0.0113\n",
            "Epoch 00007: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0759 - mean_squared_error: 0.0645 - kullback_leibler_divergence: 0.0113 - lr: 5.0000e-04\n",
            "Epoch 8/200\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.0641 - kullback_leibler_divergence: 0.0111\n",
            "Epoch 00008: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0752 - mean_squared_error: 0.0641 - kullback_leibler_divergence: 0.0111 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0745 - mean_squared_error: 0.0637 - kullback_leibler_divergence: 0.0108\n",
            "Epoch 00009: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0745 - mean_squared_error: 0.0637 - kullback_leibler_divergence: 0.0108 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0739 - mean_squared_error: 0.0634 - kullback_leibler_divergence: 0.0105\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0739 - mean_squared_error: 0.0634 - kullback_leibler_divergence: 0.0105 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.0734 - mean_squared_error: 0.0631 - kullback_leibler_divergence: 0.0103\n",
            "Epoch 00011: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0734 - mean_squared_error: 0.0631 - kullback_leibler_divergence: 0.0103 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0730 - mean_squared_error: 0.0628 - kullback_leibler_divergence: 0.0101\n",
            "Epoch 00012: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0730 - mean_squared_error: 0.0628 - kullback_leibler_divergence: 0.0101 - lr: 5.0000e-04\n",
            "Epoch 13/200\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0726 - mean_squared_error: 0.0626 - kullback_leibler_divergence: 0.0100\n",
            "Epoch 00013: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0726 - mean_squared_error: 0.0626 - kullback_leibler_divergence: 0.0100 - lr: 5.0000e-04\n",
            "Epoch 14/200\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.0722 - mean_squared_error: 0.0624 - kullback_leibler_divergence: 0.0098\n",
            "Epoch 00014: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0722 - mean_squared_error: 0.0624 - kullback_leibler_divergence: 0.0098 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0718 - mean_squared_error: 0.0622 - kullback_leibler_divergence: 0.0097\n",
            "Epoch 00015: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0718 - mean_squared_error: 0.0622 - kullback_leibler_divergence: 0.0097 - lr: 5.0000e-04\n",
            "Epoch 16/200\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0717 - mean_squared_error: 0.0621 - kullback_leibler_divergence: 0.0096\n",
            "Epoch 00016: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0717 - mean_squared_error: 0.0621 - kullback_leibler_divergence: 0.0096 - lr: 5.0000e-04\n",
            "Epoch 17/200\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0714 - mean_squared_error: 0.0619 - kullback_leibler_divergence: 0.0095\n",
            "Epoch 00017: saving model to /content/gdrive/My Drive/gdl_models/mnist_vae.hdf5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0713 - mean_squared_error: 0.0619 - kullback_leibler_divergence: 0.0095 - lr: 5.0000e-04\n",
            "Epoch 18/200\n",
            " 368/1875 [====>.........................] - ETA: 7s - loss: 0.0710 - mean_squared_error: 0.0617 - kullback_leibler_divergence: 0.0094"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}