{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN-GP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-2ipYc0wRwN",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 4: Generative Adversarial Networks\n",
        "\n",
        "## WGAN-GP\n",
        "\n",
        "In this notebook we will be investigating an extension of the WGAN known as the _Wasserstein GAN-Gradient Penalty_ (WGAN-GP). The generator in this model is exactly the same as in the WGAN model, and we make the following changes to the critic:\n",
        "\n",
        "- We include a _gradient penalty_ term in the critic loss function.\n",
        "\n",
        "- We do not clip weights of the critic.\n",
        "\n",
        "- We do not use batch normalization on the layers in the critic.\n",
        "\n",
        "### Gradient Penalty Loss\n",
        "\n",
        "The gradient penalty loss measures the mean squared norm of gradient of predictions with respect to input images and 1. Since it is intractable to take the gradient with respect to each input image, we only take the gradient with respect to a handful of points which are interpolated from real and fake images. The interpolation algorithm works as follows:\n",
        "\n",
        "1. We randomly select a batch of real and fake images, $X_\\text{real}$ and $X_\\text{fake}$ respectively, both of size $m$.\n",
        "\n",
        "2. We compute a $m$-dimensional vector $\\alpha$ whose elements are random numbers in the range [0, 1].\n",
        "\n",
        "3. For each $x_\\text{real}^i\\in X_\\text{real}$ and $x_\\text{fake}^i \\in X_\\text{fake}$ we compute\n",
        "\n",
        "  $$\\hat{x}_i = \\alpha_i x_\\text{real}^i + (\\mathbf{1} - \\alpha_i) x_\\text{fake}^i $$\n",
        "\n",
        "  where $i \\in \\{1,...,m\\}$. In other words, each $\\hat{x}_i$ is a randomly weighted interpolation between a real image, $x_\\text{real}^i$, and a fake image, $x_\\text{fake}^i$.\n",
        "\n",
        "4. For each resulting $\\hat{x}_i$, we compute $\\left\\lVert\\nabla D\\left(\\hat{x}_i\\right)\\right\\rVert$ or the L2 norm (i.e. the Euclidean norm) of the gradient of the critic at each $\\hat{x}_i$.\n",
        "\n",
        "5. We then compute the term for the loss function given by\n",
        "\n",
        "  $$ \\mathbb{E}_{\\hat{x}_i} \\big( 1 - \\left\\lVert\\nabla D\\left(\\hat{x}_i\\right)\\right\\rVert \\big)^2 $$\n",
        "\n",
        "  i.e. the mean of the difference of the L2 norm of the gradient of $D$ at $\\hat{x}_i$ and 1 squared.\n",
        "\n",
        "Another difference between the WGAN-GP and the standard WGAN is that while training the latter's critic, we freeze the weights of the generator. This is so that the generator's gradients do not contribute to the gradient penalty loss term.\n",
        "\n",
        "## Implementing a WGAN-GP\n",
        "\n",
        "Below is an example implementation of a WGAN-GP that will be used to generate images that appear to be sampled from the CelebA dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX0gPGJerkEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V75q8lO3uoGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google drive to load the data and to save the model weights.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "checkpoint_path = '/content/gdrive/My Drive/gdl_models/wgan-gp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g1-a2nPuz4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import (Input, Conv2D, LeakyReLU, Activation,\n",
        "                                     Dropout, Flatten, Dense, Reshape,\n",
        "                                     BatchNormalization, UpSampling2D,\n",
        "                                     Conv2DTranspose, Lambda)\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "def get_activation(activation_fn_name):\n",
        "  \"\"\"Get the activation layer from the name of the function.\"\"\"\n",
        "  if activation_fn_name == 'leaky_relu':\n",
        "    return LeakyReLU(alpha=0.2)\n",
        "  return Activation(activation_fn_name)\n",
        "\n",
        "\n",
        "def set_trainable(model, value):\n",
        "  \"\"\"Set each layer of a model as trainable.\"\"\"\n",
        "  model.trainable = value\n",
        "  for l in model.layers:\n",
        "    l.trainable = value\n",
        "\n",
        "\n",
        "def gradient_penalty_loss(y_true, y_pred, interpolated_values):\n",
        "  \"\"\"Compute the GP loss using interpolations of real and fake images.\"\"\"\n",
        "  gradients = K.gradients(y_pred, interpolated_values)[0]\n",
        "  gradients_sqr = K.square(gradients)\n",
        "  gradients_l2_norm = K.sqrt(K.sum(gradients_sqr,\n",
        "                                   axis=np.arange(1, len(gradients_sqr.shape))))\n",
        "  return K.mean(K.square(1 - gradients_l2_norm))\n",
        "\n",
        "\n",
        "def get_optimizer(optimizer_name, learning_rate):\n",
        "  \"\"\"Get an optimizer by name.\"\"\"\n",
        "  if optimizer_name == 'adam':\n",
        "    return Adam(lr=learning_rate)\n",
        "  if optimizer_name == 'rmsprop':\n",
        "    return RMSprop(lr=learning_rate)\n",
        "  return Adam(lr=learning_rate)\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "  \"\"\"Wasserstein loss function.\"\"\"\n",
        "  return -K.mean(y_true * y_pred)\n",
        "\n",
        "\n",
        "class WGANGP(object):\n",
        "  \"\"\"Implementation of a WGAN-GP using Keras.\"\"\"\n",
        "\n",
        "  def __init__(self, input_shape, critic_conv_filters, critic_conv_kernel_size,\n",
        "               critic_conv_strides, critic_activation, critic_dropout_rate,\n",
        "               critic_learning_rate, generator_initial_dense_layer_shape,\n",
        "               generator_upsample, generator_conv_filters,\n",
        "               generator_conv_kernel_size, generator_conv_strides,\n",
        "               generator_batch_norm_momentum, generator_activation,\n",
        "               generator_dropout_rate, generator_learning_rate, optimizer,\n",
        "               gradient_weight, z_dim, batch_size):\n",
        "    # Build the critic.\n",
        "    critic_input = Input(shape=input_shape, name='critic_input')\n",
        "    x = critic_input\n",
        "    weight_init = RandomNormal(mean=0.0, stddev=0.02)\n",
        "    for i in range(len(critic_conv_filters)):\n",
        "      x = Conv2D(filters=critic_conv_filters[i],\n",
        "                 kernel_size=critic_conv_kernel_size[i],\n",
        "                 strides=critic_conv_strides[i], padding='same',\n",
        "                 kernel_initializer=weight_init,\n",
        "                 name='critic_conv_{}'.format(i))(x)\n",
        "      x = get_activation(critic_activation)(x)\n",
        "      if critic_dropout_rate:\n",
        "        x = Dropout(rate=critic_dropout_rate)(x)\n",
        "    x = Flatten()(x)\n",
        "    critic_output = Dense(1, activation=None, kernel_initializer=weight_init)(x)\n",
        "    self.critic = Model(critic_input, critic_output)\n",
        "\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    # Build the generator.\n",
        "    generator_input = Input(shape=(z_dim,), name='generator_input')\n",
        "    x = generator_input\n",
        "    x = Dense(np.prod(generator_initial_dense_layer_shape),\n",
        "              kernel_initializer=weight_init)(x)\n",
        "    if generator_batch_norm_momentum:\n",
        "      x = BatchNormalization(momentum=generator_batch_norm_momentum)(x)\n",
        "    x = get_activation(generator_activation)(x)\n",
        "    x = Reshape(shape=generator_initial_dense_layer_shape)(x)\n",
        "    if generator_dropout_rate:\n",
        "      x = Dropout(rate=generator_dropout_rate)(x)\n",
        "    for i in range(len(generator_upsample)):\n",
        "      if generator_upsample == 2:\n",
        "        x = UpSampling2D()(x)\n",
        "        x = Conv2D(filters=generator_conv_filters[i],\n",
        "                   kernel_size=generator_conv_kernel_size[i],\n",
        "                   strides=generator_conv_strides[i], padding='same',\n",
        "                   kernel_initializer=weight_init,\n",
        "                   name='generator_conv_{}'.format(i))(x)\n",
        "      else:\n",
        "        x = Conv2DTranspose(filters=generator_conv_filters[i],\n",
        "                            kernel_size=generator_conv_kernel_size[i],\n",
        "                            strides=generator_conv_strides[i], padding='same',\n",
        "                            kernel_initializer=weight_init,\n",
        "                            name='generator_conv_{}'.format(i))(x)\n",
        "      if i == (len(generator_upsample) - 1):\n",
        "        break\n",
        "      if generator_batch_norm_momentum:\n",
        "        x = BatchNormalization(momentum=generator_batch_norm_momentum)(x)\n",
        "      x = get_activation(generator_activation)(x)\n",
        "    generator_output = Activation('tanh')(x)\n",
        "    self.generator = Model(generator_input, generator_output)\n",
        "\n",
        "    # Compile the critic.\n",
        "    set_trainable(self.generator, False)\n",
        "\n",
        "    real_img = Input(shape=input_shape)\n",
        "    z_disc = Input(shape=(z_dim,))\n",
        "    fake_img = self.generator(z_disc)\n",
        "\n",
        "    real = self.critic(real_img)\n",
        "    fake = self.critic(fake_img)\n",
        "\n",
        "    def interpolate_inputs(inputs):\n",
        "      alpha = K.random_uniform((batch_size, 1, 1, 1))\n",
        "      real, fake = inputs\n",
        "      return (alpha * real) + ((1 - alpha) * fake)\n",
        "\n",
        "    interpolated_img = Lambda(interpolate_inputs)\n",
        "    interpolated_values = self.critic(interpolated_img)\n",
        "\n",
        "    gp_loss = partial(gradient_penalty_loss,\n",
        "                      interpolated_values=interpolated_values)\n",
        "    gp_loss.__name__ = 'gradient_penalty'\n",
        "    \n",
        "    self.critic_model = Model(inputs=[real_img, z_disc],\n",
        "                              outputs=[real, fake, interpolated_values])\n",
        "    self.critic_model.compile(\n",
        "        loss=[wasserstein_loss, wasserstein_loss, gp_loss],\n",
        "        optimizer=get_optimizer(optimizer, critic_learning_rate),\n",
        "        loss_weights=[1, 1, gradient_weight])\n",
        "\n",
        "    set_trainable(self.generator, True)\n",
        "\n",
        "    # Compile the generator.\n",
        "    set_trainable(self.critic, False)\n",
        "\n",
        "    model_input = Input(shape=(z_dim,))\n",
        "    model_output = self.critic(self.generator(model_input))\n",
        "    self.model = Model(model_input, model_output)\n",
        "    self.model.compile(\n",
        "        loss=wasserstein_loss,\n",
        "        optimizer=get_optimizer(optimizer, generator_learning_rate))\n",
        "\n",
        "    set_trainable(self.critic, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SksqY0wHcbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE = 64\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "wgangp = WGANGP(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                critic_conv_filters=(64, 128, 256, 512),\n",
        "                critic_conv_kernel_size=(5, 5, 5, 5),\n",
        "                critic_conv_strides=(2, 2, 2, 2),\n",
        "                critic_activation='leaky_relu',\n",
        "                critic_dropout_rate=0.4, critic_learning_rate=0.0002,\n",
        "                generator_initial_dense_layer_shape=(4, 4, 64),\n",
        "                generator_upsample=(1, 1, 1, 1),\n",
        "                generator_conv_filters=(256, 128, 64, 3),\n",
        "                generator_conv_kernel_size=(5, 5, 5, 5),\n",
        "                generator_conv_strides=(2, 2, 2, 2),\n",
        "                generator_batch_norm_momentum=0.9,\n",
        "                generator_activation='relu',\n",
        "                generator_dropout_rate=None,\n",
        "                generator_learning_rate=0.0002,\n",
        "                optimizer='adam',\n",
        "                gradient_weight=10,\n",
        "                z_dim=100,\n",
        "                batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYUp7lvTHVdX",
        "colab_type": "text"
      },
      "source": [
        "TODO Add model summary\n",
        "\n",
        "## Training the WGAN-GP\n",
        "\n",
        "## Analyzing the Results"
      ]
    }
  ]
}