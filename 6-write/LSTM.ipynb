{"cells":[{"cell_type":"markdown","metadata":{"id":"HgNvLVLb8H89","colab_type":"text"},"source":["# Chapter 6: Text\n","\n","## Long Short-Term Memory Networks\n","\n","A _long short-term memory_ (LSTM) network is a particular type of layer in _recurrent neural networks_ (RNNs), a type of neural network which processes sequential data that is discretely separated over steps in the \"time\" dimension. An LSTM network is an RNN which has an LSTM layer.\n","\n","A vanilla recurrent layer computes its output for each timestep, $\\mathbf{h}_{(t)}$, using the values of the input sequence at the current timestemp, $\\mathbf{x}_{(t)}$, and the output of the layer at the previous timestep, $\\mathbf{h}_{(t-1)}$. LSTM cells also contain a state vector for each timestep, $\\mathbf{c}_{(t)}$, which it with $\\mathbf{h}_{(t)}$ and $\\mathbf{x}_{(t)}$ to compute the new layer state and output at each timestep. Below is a diagram of an LSTM cell."]},{"cell_type":"markdown","metadata":{"id":"gkGcWEDy1Gb4","colab_type":"text"},"source":["<img width=\"600\" src=\"https://camo.githubusercontent.com/c433cc6abd96207bc5e01ad4253026152785b9f9/68747470733a2f2f692e696d6775722e636f6d2f434f32554e4c5a2e706e67\">"]},{"cell_type":"markdown","metadata":{"id":"1okxuLUW1BjG","colab_type":"text"},"source":["For an in-depth discussion of RNNs, see [`RecurrentNeuralNetworks.ipynb`](https://github.com/DCtheTall/hands-on-machine-learning/blob/master/chapter14/RecurrentNeuralNetworks.ipynb) in my GitHub repository for my implementations of _Hands On Machine Learning with Scikit-Learn and TensorFlow_ by Aurélian Géron."]}],"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"authorship_tag":"ABX9TyN8cOfNPd64yFFz9B4eIWw/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}