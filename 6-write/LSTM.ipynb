{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgNvLVLb8H89",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 6: Text\n",
        "\n",
        "## Long Short-Term Memory Networks\n",
        "\n",
        "A _long short-term memory_ (LSTM) network is a particular type of layer in _recurrent neural networks_ (RNNs), a type of neural network which processes sequential data that is discretely separated over steps in the \"time\" dimension. An LSTM network is an RNN which has an LSTM layer.\n",
        "\n",
        "A vanilla recurrent layer computes its output for each timestep, $\\mathbf{h}_{(t)}$, using the values of the input sequence at the current timestemp, $\\mathbf{x}_{(t)}$, and the output of the layer at the previous timestep, $\\mathbf{h}_{(t-1)}$. LSTM cells also contain a state vector for each timestep, $\\mathbf{c}_{(t)}$, which it with $\\mathbf{h}_{(t)}$ and $\\mathbf{x}_{(t)}$ to compute the new layer state and output at each timestep. Below is a diagram of an LSTM cell.\n",
        "\n",
        "<img width=\"600\" src=\"https://camo.githubusercontent.com/c433cc6abd96207bc5e01ad4253026152785b9f9/68747470733a2f2f692e696d6775722e636f6d2f434f32554e4c5a2e706e67\">\n",
        "\n",
        "For an in-depth discussion of RNNs, see [`RecurrentNeuralNetworks.ipynb`](https://github.com/DCtheTall/hands-on-machine-learning/blob/master/chapter14/RecurrentNeuralNetworks.ipynb) in my GitHub repository for my implementations of _Hands On Machine Learning with Scikit-Learn and TensorFlow_ by Aurélian Géron.\n",
        "\n",
        "## Downloading the Data\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "The first step is to dowload the data and to _tokenize_ the text, i.e. split up the text into individual units. In this case, we will be separating the text into lowercase words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKxfx73oRflO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "15c8b884-4e30-46cb-d5d9-a7d133b056ef"
      },
      "source": [
        "!wget http://www.gutenberg.org/cache/epub/11339/pg11339.txt && \\\n",
        "  mv pg11339.txt aesop.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-26 18:40:56--  http://www.gutenberg.org/cache/epub/11339/pg11339.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 243023 (237K) [text/plain]\n",
            "Saving to: ‘pg11339.txt’\n",
            "\n",
            "pg11339.txt         100%[===================>] 237.33K   498KB/s    in 0.5s    \n",
            "\n",
            "2020-04-26 18:41:10 (498 KB/s) - ‘pg11339.txt’ saved [243023/243023]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B7SGclKTrK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "filename = 'aesop.txt'\n",
        "with open(filename, encoding='utf-8-sig') as f:\n",
        "  text = f.read()\n",
        "\n",
        "seq_length = 20\n",
        "start_story = '| ' * seq_length\n",
        "\n",
        "start = text.find(\"THE FOX AND THE GRAPES\\n\\n\\n\")\n",
        "end = text.find(\"ILLUSTRATIONS\\n\\n\\n[\")\n",
        "\n",
        "text = text[start:end]\n",
        "text = text.lower()\n",
        "text = start_story + text\n",
        "text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
        "text = text.replace('\\n', ' ')\n",
        "text = re.sub('  +', '. ', text).strip()\n",
        "text = text.replace('..', '.')\n",
        "text = text.replace('..', '.')\n",
        "text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
        "text = re.sub('\\s{2,}', ' ', text)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False, filters='')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "tokens_list = tokenizer.texts_to_sequences([text])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUWURFFLYZds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26188a91-7416-4e05-ae45-aa4524361f63"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "213714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBq9ztmDa2sB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ac71f78-9650-4fab-d517-050cc74fbf41"
      },
      "source": [
        "total_words"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofaY6fMtVzM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6e5a244e-aaf3-49d5-ef23-e6c01af2b512"
      },
      "source": [
        "text[:1000]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' | | | | | | | | | | | | | | | | | | | | the fox and the grapes . a hungry fox saw some fine bunches of grapes hanging from a vine that was trained along a high trellis , and did his best to reach them by jumping as high as he could into the air . but it was all in vain , for they were just out of reach : so he gave up trying , and walked away with an air of dignity and unconcern , remarking , \" i thought those grapes were ripe , but i see now they are quite sour . \" | | | | | | | | | | | | | | | | | | | | the goose that laid the golden eggs . a man and his wife had the good fortune to possess a goose which laid a golden egg every day . lucky though they were , they soon began to think they were not getting rich fast enough , and , imagining the bird must be made of gold inside , they decided to kill it in order to secure the whole store of precious metal at once . but when they cut it open they found it was just like any other goose . thus , they neither got rich all at once , as they'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2Ew4rOYWl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "23ae32ed-11b3-4b21-949f-3df7a3538536"
      },
      "source": [
        "', '.join(str(t) for t in tokens_list[:1000])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 56, 4, 3, 940, 5, 6, 382, 56, 94, 77, 216, 1557, 9, 940, 941, 62, 6, 581, 20, 12, 2226, 162, 6, 359, 2227, 2, 4, 158, 11, 250, 7, 383, 35, 29, 1176, 25, 359, 25, 10, 88, 55, 3, 582, 5, 19, 16, 12, 37, 14, 785, 2, 17, 23, 47, 96, 43, 9, 383, 30, 28, 10, 170, 36, 425, 2, 4, 426, 89, 21, 57, 582, 9, 1558, 4, 2228, 2, 1559, 2, 8, 18, 144, 260, 940, 47, 1177, 2, 19, 18, 90, 115, 23, 63, 360, 2229, 5, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1178, 20, 519, 3, 659, 660, 5, 6, 66, 4, 11, 520, 32, 3, 91, 384, 7, 2230, 6, 1178, 48, 519, 6, 659, 1179, 159, 75, 5, 942, 385, 23, 47, 2, 23, 171, 126, 7, 204, 23, 47, 45, 386, 474, 361, 177, 2, 4, 2, 2231, 3, 229, 186, 34, 109, 9, 786, 427, 2, 23, 943, 7, 237, 16, 14, 309, 7, 944, 3, 583, 584, 9, 2232, 2233, 24, 78, 5, 19, 26, 23, 387, 16, 787, 23, 122, 16, 12, 96, 103, 132, 95, 1178, 5, 261, 2, 23, 945, 86, 474, 37, 24, 78, 2, 25, 23, 32, 1560, 2, 788, 946, 132, 292, 3, 789, 2234, 7, 50, 1180, 5, 93, 428, 104, 4, 2235, 37, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 197, 4, 3, 337, 5, 69, 12, 78, 6, 207, 20, 12, 2236, 21, 337, 5, 6, 197, 181, 9, 74, 2, 4, 22, 7, 277, 2, 8, 338, 3, 198, 17, 44, 2, 8, 4, 87, 53, 81, 4, 116, 36, 40, 1181, 14, 3, 207, 2, 4, 111, 3, 337, 31, 29, 31, 4, 790, 35, 5, 24, 112, 3, 337, 88, 429, 16, 51, 292, 2, 4, 23, 251, 7, 119, 7, 50, 1182, 4, 388, 69, 5, 8, 338, 1561, 2, 8, 22, 3, 197, 7, 277, 30, 8, 3, 101, 339, 7, 67, 42, 7, 2237, 35, 43, 29, 6, 585, 5, 8, 28, 53, 1562, 6, 138, 2, 4, 71, 586, 36, 3, 791, 4, 113, 277, 2238, 84, 29, 40, 792, 262, 62, 6, 2239, 2, 4, 793, 7, 34, 230, 5, 29, 4, 29, 6, 139, 2240, 43, 4, 94, 3, 197, 941, 69, 5, 8, 947, 60, 8, 16, 140, 2, 8, 587, 80, 1183, 2, 1184, 2, 51, 588, 30, 19, 13, 182, 310, 199, 55, 6, 1563, 9, 389, 941, 69, 2, 54, 13, 103, 2, 362, 13, 278, 475, 172, 390, 1185, 391, 13, 5, 8, 5, 54, 13, 63, 1186, 13, 278, 34, 1564, 29, 3, 2241, 1187, 9, 5, 260, 476, 13, 49, 78, 122, 7, 34, 1565, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1566, 97, 5, 69, 12, 78, 6, 97, 38, 263, 7, 1188, 24, 311, 4, 661, 35, 133, 132, 2242, 2, 4, 38, 12, 6, 166, 1189, 7, 159, 31, 38, 64, 7, 11, 1190, 207, 5, 28, 11, 141, 1567, 6, 794, 208, 11, 312, 7, 1191, 311, 9, 11, 589, 5, 3, 97, 12, 80, 1192, 9, 3, 794, 2, 4, 1568, 68, 1569, 16, 21, 2243, 1570, 5, 19, 57, 145, 97, 64, 36, 7, 15, 4, 22, 2, 8, 3, 2244, 1187, 13, 183, 199, 3, 173, 2, 41, 117, 5, 13, 156, 204, 2, 67, 13, 2, 20, 46, 794, 12, 590, 13, 25, 6, 795, 9, 2245, 65, 27, 3, 2246, 2, 16, 42, 6, 2247, 9, 2248, 5, 8, 5, 2249, 42, 363, 1193, 17, 2250, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 948, 39, 1194, 4, 3, 1195, 5, 69, 12, 78, 6, 948, 39, 1194, 38, 392, 4, 1571, 29, 73, 5, 6, 1195, 2, 174, 2, 313, 7, 92, 4, 1196, 14, 3, 209, 1572, 33, 4, 3, 948, 39, 1194, 2, 264, 109, 11, 949, 4, 477, 10, 12, 57, 2251, 478, 9, 430, 2, 127, 15, 54, 10, 76, 92, 4, 479, 11, 207, 30, 8, 105, 178, 98, 7, 217, 31, 184, 173, 20, 108, 2, 8, 10, 22, 2, 8, 4, 2, 1197, 2, 210, 2252, 2253, 79, 34, 2254, 5, 8, 3, 1195, 950, 15, 2, 19, 85, 2, 8, 18, 393, 204, 9, 16, 2, 265, 30, 118, 2, 796, 18, 119, 211, 1198, 7, 2255, 76, 34, 2256, 14, 51, 61, 29, 46, 948, 5, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 337, 14, 1199, 5, 78, 72, 6, 61, 37, 3, 337, 218, 142, 14, 1199, 2, 4, 2257, 3, 250, 340, 9, 2258, 266, 279, 3, 1200, 9, 3, 197, 5, 110, 394, 2259, 32, 120, 2260, 2, 6, 139, 9, 77, 521, 4, 1573, 86, 36, 4, 22, 2, 8, 18, 204, 18, 49, 522, 72, 6, 591, 48, 79, 2261, 210, 951, 14, 3, 952, 2, 797, 13, 2262, 4, 280, 16, 43, 5, 16, 42, 20, 105, 114, 2263, 6, 794, 208, 3, 312, 9, 210, 395, 3, 197, 2, 48, 79, 29, 128, 1569, 1191, 172, 9, 40, 953, 5, 8, 74, 2264, 12, 1201, 2265, 2, 4, 16, 32, 120, 1202, 943, 7, 2266, 16, 2, 26, 57, 145, 139, 86, 72, 11, 523, 4, 22, 2, 8, 18, 2267, 21, 13, 37, 20, 3, 591, 99, 172, 42, 57, 2268, 31, 30, 19, 182, 18, 431, 38, 42, 163, 7, 794, 3, 197, 65, 8, 1, 1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGxc8xqGYqs1",
        "colab_type": "text"
      },
      "source": [
        "### Building the Dataset\n",
        "\n",
        "We want to use the LSTM network to predict the next word in the sequence. In order to do so, we will train the network with sequences of 20 words. The output of each sequence is the subsequent word.\n",
        "\n",
        "Below is code which generates the dataset from the tokens list constructed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW2PGVvaZPHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9629f82-28e0-491b-b3f5-2becc2d8dc38"
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "def generate_sequences(tokens_list, step, total_words):\n",
        "  \"\"\"Generate a dataset from a tokenized body of text.\"\"\"\n",
        "  X, y = [], []\n",
        "  for i in range(0, len(tokens_list) - seq_length, step):\n",
        "    X.append(tokens_list[i:i + seq_length])\n",
        "    y.append(tokens_list[i + seq_length])\n",
        "  y = np_utils.to_categorical(y, num_classes=total_words)\n",
        "  num_seq = len(X)\n",
        "  print('Number of sequences:', num_seq)\n",
        "  return np.array(X), np.array(y), num_seq"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73OsApj0bGr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2199d146-619a-4d22-f52a-070da4147c66"
      },
      "source": [
        "step = 1\n",
        "seq_length = 20\n",
        "X, y, num_seq = generate_sequences(tokens_list, step, total_words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 50415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYkbEw4obdWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08f8097b-0505-4a18-81c3-f689250181cb"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50415, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM9CHiDkcKp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0e1f29b-134c-4b79-cda2-aa1b6a73be07"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50415, 4169)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lic91gfrcO1p",
        "colab_type": "text"
      },
      "source": [
        "## The Embedding Layer\n",
        "\n",
        "The _embedding layer_ functions as a lookup table that converts each token to a vector of a specified `embedding_size`. The number of weights learned is equal to the size of the vocabulary multiplied by the `embedding_size`. Transforming each token into a continuously valued vector enables the model to learn a way to represent each word using backpropagation.\n",
        "\n",
        "## Building the LSTM Network\n",
        "\n",
        "Below is code for creating an LSTM model using TensorFlow and Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJiwftJKnA0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "n_units = 256\n",
        "embedding_size = 100\n",
        "\n",
        "text_in = Input(shape=(None,))\n",
        "x = Embedding(total_words, embedding_size)(text_in)\n",
        "x = LSTM(n_units)(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "text_out = Dense(total_words, activation='softmax')(x)\n",
        "\n",
        "model = Model(text_in, text_out)\n",
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNmhnbfEXZXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ce7f1271-b930-40e6-b6ed-d29374f1cb48"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         416900    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               365568    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4169)              1071433   \n",
            "=================================================================\n",
            "Total params: 1,853,901\n",
            "Trainable params: 1,853,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5am_GYctHzi",
        "colab_type": "text"
      },
      "source": [
        "Now we will train the model for 100 epochs and save the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D92RrzystKPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "checkpoint_path = '/content/gdrive/My Drive/gdl_models/lstm/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r0YaizbtmuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "callbacks = [ModelCheckpoint(filepath=checkpoint_path + 'weights.hdf5',\n",
        "                             verbose=1, save_weights_only=True),\n",
        "             ModelCheckpoint(\n",
        "                 filepath=checkpoint_path + 'weights_{epoch:04d}.hdf5',\n",
        "                 verbose=1, save_weights_only=True)]\n",
        "\n",
        "model.fit(X, y, epochs=1000, batch_size=32, shuffle=True, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSxcuO2IW1OM",
        "colab_type": "text"
      },
      "source": [
        "## Generating New Text\n",
        "\n",
        "Now we will have the model generate a new sequence of words. We do so by first giving the model an input sequence and then letting it predict the next word. We then use the include the new word in the following sequence and repeat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HL4wTdMXjE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(checkpoint_path + 'weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyZKQt-5Xl6X",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, the model will predict the probability that each word in the vocabulary follows the sequence. We can introduce a _temperature_ parameter to scale the output probabilities. A lower temperature means the model will be more deterministic, i.e. more likely to always pick the word with just the highest probability according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF50hwF5XlGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "  \"\"\"Sample from the probabilities predicted by the model with temperature.\"\"\"\n",
        "  preds = np.log(np.asarray(preds).astype('float64')) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  return np.argmax(np.random.multinomial(1, preds, 1))\n",
        "\n",
        "\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len, temperature):\n",
        "  \"\"\"Generate new text using the trained model.\"\"\"\n",
        "  output = seed_text\n",
        "  seed_text = start_story + seed_text\n",
        "  for _ in range(next_words):\n",
        "    tokens_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = tokens_list[-max_sequence_len:]\n",
        "    y_class = sample_with_temperature(model.predict([token_list], verbose=0)[0],\n",
        "                                      temperature)\n",
        "    if y_class == 0:\n",
        "      output_word = ''\n",
        "    else:\n",
        "      output_word = tokenizer.index_word[y_class]\n",
        "    if output_word == '|':\n",
        "      break\n",
        "    output += output_word + ' '\n",
        "    seed_text += output_word + ' '\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTZTjKYLb_kO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "357983b1-84a0-423c-9ee2-cb6d49f43257"
      },
      "source": [
        "seed_text = 'the frog and the snake '\n",
        "gen_words = 500\n",
        "temp = 0.1\n",
        "\n",
        "generate_text(seed_text, gen_words, model, seq_length, temp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the frog and the snake . a fox , they to a lion so used to herself and flew up in the the own , and begged to be he . with his other not the a lion , so much that he was going to so a beasts and by one . when the old lion again was about it , with his back , and said to him the other , \" all i be , came to a - - when you are : i do you all any saying the lion , for all that , i will be food . the woman and killed one of them what it all the - and was as i - to be never - ass . \" as he was the man of the at home in the again , as he said , \" my friend . if you mouse , this , however , i have been not a , friend of the , we did all . \" what you have know some time , as he are , and said to him , \" all being get out ; for the but had good deal from the was from the beasts , and \" on the fine old . she was so much that they got the head out of them , and see now this day , the for one day for the good was not be began to i . \" but one of the wolf said , \" you if you do that , which the envy you have only don\\'t get on you more to do so . \" '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGpQg-oM4so4",
        "colab_type": "text"
      },
      "source": [
        "Below is code for generating human-led text. The model picks the top 10 words with the highest probability of coming next and a human user decides what the model writes. This is similar to how automatic suggestions work in messaging apps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45FprEjE43yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def generate_human_led_text(model, max_sequence_len):\n",
        "  \"\"\"Generate a human-led sequence of text.\"\"\"\n",
        "  output = ''\n",
        "  seed_text = start_story\n",
        "  while True:\n",
        "    tokens_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = tokens_list[-max_sequence_len:]\n",
        "    probs = model.predict([token_list])[0]\n",
        "    top_10_idx = np.flip(np.argsort(probs)[-10:])\n",
        "    top_10_probs = probs[top_10_idx]\n",
        "    top_10_words = tokenizer.sequences_to_texts([[x] for x in top_10_idx])\n",
        "    for prob, word in zip(top_10_probs, top_10_words):\n",
        "      print('{:<6.1%} : {}'.format(prob, word))\n",
        "    chosen_word = input()\n",
        "    if chosen_word == '|':\n",
        "      break\n",
        "    seed_text += chosen_word + ' '\n",
        "    output += chosen_word + ' '\n",
        "    clear_output()\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYHYGDTQ6WxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_human_led_text(model, seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}