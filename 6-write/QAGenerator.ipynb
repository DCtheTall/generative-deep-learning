{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QAGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZbTWlcNp2Dn",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 6: Write\n",
        "\n",
        "## Question-Answer Generator\n",
        "\n",
        "The goal of this notebook is to create a model which can generate question and answer pairs about a block of text. This project is based on the [`qgen-workshop` TensorFlow codebase](https://github.com/Maluuba/qgen-workshop). The model consists of two components:\n",
        "\n",
        "- An RNN which identifies possible question answers from a block of text.\n",
        "\n",
        "- An encoder-decoder network that generates possible questions that the answers identified by the former model could be for.\n",
        "\n",
        "An _encoder-decoder_ network is a type of RNN that outputs a new sequence from its input. Some applications of encoder-decoder networks include machine translation, question generation, and text summarization. An encoder-decoder model trains an encoder RNN to encode the input sequence into a vector input for the decoder RNN which outputs a novel sequence from the vector input.\n",
        "\n",
        "## Question-Answer Dataset\n",
        "\n",
        "### Step 1: Downloading the data from the Maluuba News QA repository\n",
        "\n",
        "Below we download and preprocess the data for the model. The data we are using for this notebook is provided by the [Maluuba News QA GitHub repository](https://github.com/Maluuba/newsqa). We use the manual setup instructions with the relevant code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuanHAf9txrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount drive.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "base_dir = '/content/gdrive/My Drive/gdl_models/qa/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6J2MkhrsRcs",
        "colab_type": "code",
        "outputId": "a15d6043-90be-429d-95fe-237ccce39697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/Maluuba/newsqa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'newsqa'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 132 (delta 4), reused 4 (delta 2), pack-reused 119\u001b[K\n",
            "Receiving objects: 100% (132/132), 610.20 KiB | 2.45 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQkI4Al_tPuF",
        "colab_type": "text"
      },
      "source": [
        "For legal reasons, you have to download the data yourself on [Microsoft's website](https://msropendata.com/datasets/939b1042-6402-4697-9c15-7a28de7e1321) and state what you are using the data for. To save myself the trouble of having to reupload the data each time we get a new Colab kernel, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vik_Bn7ewtV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def load_or_copy(filename):\n",
        "  \"\"\"Load a file from Drive or copy it locally into Drive.\"\"\"\n",
        "  drive_path = base_dir + filename\n",
        "  if os.path.isfile(drive_path):\n",
        "    print('File exists in drive')\n",
        "    subprocess.call(['cp', drive_path, '.'])\n",
        "  else:\n",
        "    print('File exists locally')\n",
        "    subprocess.call(['cp', filename, drive_path])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMLD1-5iOhO",
        "colab_type": "code",
        "outputId": "11740cf7-ac8a-40e0-d541-edc73454b2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "load_or_copy('newsqa.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists in drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEaDd-xeifBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv newsqa.tar.gz newsqa/maluuba/newsqa/newsqa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZQIuS1z5am",
        "colab_type": "text"
      },
      "source": [
        "Now we also download the CNN stories we will use to train the model. You can download the stories [here](https://cs.nyu.edu/~kcho/DMQA/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEu_ZT0GjfpE",
        "colab_type": "code",
        "outputId": "625c3b7c-15de-4c3d-d962-90471d672fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "load_or_copy('cnn_stories.tgz')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists in drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U-0iyWOkrbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv cnn_stories.tgz newsqa/maluuba/newsqa/cnn_stories.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59dHsLC39-b",
        "colab_type": "text"
      },
      "source": [
        "Now we upload the Java dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGAKCBMEa_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImAW_vR84MBt",
        "colab_type": "code",
        "outputId": "1d38d3fd-6890-4836-b93a-8e923fc9fcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "load_or_copy('stanford-postagger-2015-12-09.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists in drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G-nUqCN-wP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r stanford-postagger-2015-12-09.zip newsqa/maluuba/newsqa/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Cylm1voVWf",
        "colab_type": "text"
      },
      "source": [
        "Now we run the data processing script in the repository. We run the repository's tests to make sure the data was processed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RGUzWN0td9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd newsqa && python2 maluuba/newsqa/data_generator.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx-gzrvN1kyt",
        "colab_type": "code",
        "outputId": "d8f808b4-4e03-4ee1-e928-8b8496cdeb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!cd newsqa && python2 -m unittest discover ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 2020-05-04 00:25:51,936 - data_processing.py::__init__\n",
            "Loading dataset from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2020-05-04 00:25:51,936 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2020-05-04 00:25:52,456 - data_processing.py::__init__\n",
            "Loading stories from `/content/newsqa/maluuba/newsqa/cnn_stories.tgz`...\n",
            "Getting story texts: 100% 12.7k/12.7k [00:12<00:00, 1.05k stories/s] \n",
            "Setting story texts: 100% 120k/120k [00:03<00:00, 37.0k questions/s] \n",
            "[INFO] 2020-05-04 00:26:07,792 - data_processing.py::__init__\n",
            "Done loading dataset.\n",
            "Checking for possible corruption: 100% 120k/120k [00:01<00:00, 103k questions/s]\n",
            ".[INFO] 2020-05-04 00:26:09,045 - data_processing.py::dump\n",
            "Packaging dataset to `/content/newsqa/combined-newsqa-data-v1.json`.\n",
            "Building json: 100% 120k/120k [00:05<00:00, 21.7k questions/s] \n",
            "Checking for possible corruption: 100% 12.7k/12.7k [00:00<00:00, 18.7k stories/s]\n",
            "Gathering answers: 100% 120k/120k [00:01<00:00, 107k questions/s]\n",
            "..[INFO] 2020-05-04 00:26:27,733 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/combined-newsqa-data-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 58.6k questions/s]\n",
            "Comparing stories: 100% 120k/120k [00:30<00:00, 3.93k rows/s]\n",
            ".[INFO] 2020-05-04 00:27:03,871 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-tokenized-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 58.3k questions/s]\n",
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 12 tests in 78.046s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqIW19KgZLZ-",
        "colab_type": "text"
      },
      "source": [
        "Now let's save the data to Drive. Once the code below runs, we will have a copy of the data that'll persist through different Colab kernel sessions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5vEtQw8Fp9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upload_to_drive(filepath):\n",
        "  \"\"\"Copy a file to Drive.\"\"\"\n",
        "  drive_path = base_dir + filepath.split('/')[-1]\n",
        "  subprocess.call(['cp', filepath, drive_path])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x__nql9IZeoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('newsqa/split_data/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfDRjztAZrnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('newsqa/split_data/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG3h7uLBZu6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('newsqa/split_data/dev.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHs9v9ltxWaw",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Download GloVe embeddings\n",
        "\n",
        "We need to download a set of pretrained word embeddings that were created using the Stanford GloVe (Global Vectors) algorithm, an unsupervised learning algorithm that uses a large set of words.\n",
        "\n",
        "You can run the code below to download the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSqzvR5HDI3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "234bf9a4-617a-4d9a-9ac3-d22551093309"
      },
      "source": [
        "!mkdir -p glove && wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 02:55:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-07 02:56:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-07 02:56:00--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 26s  \n",
            "\n",
            "2020-05-07 03:02:26 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJGTszJqEF_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('glove.6B.zip', 'r') as z:\n",
        "    z.extractall('glove')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tLHS5rFF5T3",
        "colab_type": "text"
      },
      "source": [
        "I'll upload the 100-dimensional embeddings we'll be using for the model to Drive so I don't have to download the entire zip file again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A12RPPJ6Fo6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('glove/glove.6B.100d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX926FhOGI1a",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to trim the embeddings down to only the relevant encodings for our training set are kept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhlqqcp4HYto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "MAX_DOCUMENT_LENGTH = 200\n",
        "\n",
        "\n",
        "def tokenize(words):\n",
        "  \"\"\"Transform a string of words into uniformly lower case tokens.\"\"\"\n",
        "  return [w.lower() for w in words.split(' ')]\n",
        "\n",
        "\n",
        "def load_data(csv_path):\n",
        "  \"\"\"Load the data from the CSV files generated by the Maluuba News QA repository.\"\"\"\n",
        "  stories = {}\n",
        "  with open(csv_path) as f:\n",
        "    for i, row in enumerate(csv.reader(f)):\n",
        "      if i == 0:  # Header row.\n",
        "        continue\n",
        "\n",
        "      doc_id = row[0]\n",
        "      existing_stories = stories.setdefault(doc_id, [])\n",
        "\n",
        "      doc_text = row[1]\n",
        "      if existing_stories and doc_text == existing_stories[0]['document_text']:\n",
        "        doc_words = existing_stories[0]['document_words']\n",
        "      else:\n",
        "        doc_words = tokenize(doc_text)[:MAX_DOCUMENT_LENGTH]\n",
        "\n",
        "      q_text = row[2]\n",
        "      q_words = tokenize(q_text)\n",
        "\n",
        "      answer = row[3]\n",
        "      ans_indices = []\n",
        "      for chunk in answer.split(','):\n",
        "        start, end = (int(idx) for idx in chunk.split(':'))\n",
        "        if end < MAX_DOCUMENT_LENGTH:\n",
        "          ans_indices.extend(range(start, end))\n",
        "      ans_text = ' '.join(doc_words[i] for i in ans_indices)\n",
        "\n",
        "      if ans_indices:\n",
        "        existing_stories.append({\n",
        "            'document_id': doc_id,\n",
        "            'document_text': doc_text,\n",
        "            'document_words': doc_words,\n",
        "            'answer_text': ans_text,\n",
        "            'answer_indices': ans_indices,\n",
        "            'question_text': q_text,\n",
        "            'question_words': q_words,\n",
        "        })\n",
        "  return stories\n",
        "\n",
        "\n",
        "def trim_embeddings():\n",
        "  \"\"\"Trim the Stanford GloVe embeddings to the 10,000 most common words in the training set.\"\"\"\n",
        "  doc_counts = Counter()\n",
        "  q_counts = Counter()\n",
        "\n",
        "  for data in [load_data(base_dir + 'train.csv').values(),\n",
        "               load_data(base_dir + 'test.csv').values()]:\n",
        "    for stories in data:\n",
        "      if stories:\n",
        "        doc_counts.update(stories[0]['document_words'])\n",
        "        for story in stories:\n",
        "          q_counts.update(story['question_words'])\n",
        "  \n",
        "  keep = set()\n",
        "  for word, _ in q_counts.most_common(5000):\n",
        "    keep.add(word)\n",
        "  for word, _ in doc_counts.most_common():\n",
        "    if len(keep) >= 10000:\n",
        "      break\n",
        "    keep.add(word)\n",
        "\n",
        "  with open(base_dir + 'glove.6B.100d.txt') as f:\n",
        "    with open(base_dir + 'glove.6B.100d.trimmed.txt', 'w') as f2:\n",
        "      for line in f:\n",
        "        if line.split(' ')[0] in keep:\n",
        "          f2.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vKKzo8QzM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trim_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1kZ_sCJ9fJ",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Preprocessing the data for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siYBTQFVxsQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "START = '<START>'\n",
        "END = '<END>'\n",
        "MAX_BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "def lookup_word():\n",
        "  \"\"\"\"\"\"\n",
        "  pass\n",
        "\n",
        "\n",
        "def prepare_batch(batch):\n",
        "  \"\"\"\"\"\"\n",
        "  ids_to_indices = {}\n",
        "  doc_ids = []\n",
        "  doc_text = []\n",
        "  doc_words = []\n",
        "  ans_text = []\n",
        "  ans_indices = []\n",
        "  q_text = []\n",
        "  q_input_words = []\n",
        "  q_output_words = []\n",
        "  for i, entry in enumerate(batch):\n",
        "    ids_to_indices.setdefault(entry['document_id'], []).append(i)\n",
        "    doc_ids.append(entry['document_id'])\n",
        "    doc_text.append(entry['document_text'])\n",
        "    doc_words.append(entry['document_words'])\n",
        "    ans_text.append(entry['answer_text'])\n",
        "    ans_indices.append(entry['answer_indices'])\n",
        "    q_text.append(entry['question_text'])\n",
        "    q_words = entry['question_words']\n",
        "    q_input_words = [[START] + q_words]\n",
        "    q_output_words = [q_words + [END]]\n",
        "\n",
        "  batch_size = len(batch)\n",
        "  max_doc_len = max((len(doc) for doc in doc_words), default=0)\n",
        "  max_ans_len = max((len(ans) for ans in ans_indices), default=0)\n",
        "  max_q_len = max((len(q) for q in q_input_words), default=0)\n",
        "\n",
        "\n",
        "def process_stories(stories):\n",
        "  \"\"\"\"\"\"\n",
        "  batch = []\n",
        "  vals = list(stories.values())\n",
        "  random.shuffle(vals)\n",
        "  for story in vals:\n",
        "    if len(batch) + len(story) > MAX_BATCH_SIZE:\n",
        "      yield prepare_batch(batch)\n",
        "      batch = []\n",
        "    batch.extend(story)\n",
        "  if batch:\n",
        "    yield prepare_batch(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}