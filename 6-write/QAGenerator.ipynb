{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QAGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZbTWlcNp2Dn",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 6: Write\n",
        "\n",
        "## Question-Answer Generator\n",
        "\n",
        "The goal of this notebook is to create a model which can generate question and answer pairs about a block of text. This project is based on the [`qgen-workshop` TensorFlow codebase](https://github.com/Maluuba/qgen-workshop). The model consists of two components:\n",
        "\n",
        "- An RNN which identifies possible question answers from a block of text.\n",
        "\n",
        "- An encoder-decoder network that generates possible questions that the answers identified by the former model could be for.\n",
        "\n",
        "An _encoder-decoder_ network is a type of RNN that outputs a new sequence from its input. Some applications of encoder-decoder networks include machine translation, question generation, and text summarization. An encoder-decoder model trains an encoder RNN to encode the input sequence into a vector input for the decoder RNN which outputs a novel sequence from the vector input.\n",
        "\n",
        "## Question-Answer Dataset\n",
        "\n",
        "### Step 1: Downloading the data from the Maluuba News QA repository\n",
        "\n",
        "Below we download and preprocess the data for the model. The data we are using for this notebook is provided by the [Maluuba News QA GitHub repository](https://github.com/Maluuba/newsqa). We use the manual setup instructions with the relevant code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuanHAf9txrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount drive.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "base_dir = '/content/gdrive/My Drive/gdl_models/qa/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6J2MkhrsRcs",
        "colab_type": "code",
        "outputId": "a15d6043-90be-429d-95fe-237ccce39697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/Maluuba/newsqa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'newsqa'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 132 (delta 4), reused 4 (delta 2), pack-reused 119\u001b[K\n",
            "Receiving objects: 100% (132/132), 610.20 KiB | 2.45 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQkI4Al_tPuF",
        "colab_type": "text"
      },
      "source": [
        "For legal reasons, you have to download the data yourself on [Microsoft's website](https://msropendata.com/datasets/939b1042-6402-4697-9c15-7a28de7e1321) and state what you are using the data for. To save myself the trouble of having to reupload the data each time we get a new Colab kernel, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vik_Bn7ewtV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def load_or_copy(filename):\n",
        "  \"\"\"Load a file from Drive or copy it locally into Drive.\"\"\"\n",
        "  drive_path = base_dir + filename\n",
        "  if os.path.isfile(drive_path):\n",
        "    print('File exists in drive')\n",
        "    subprocess.call(['cp', drive_path, '.'])\n",
        "  else:\n",
        "    print('File exists locally')\n",
        "    subprocess.call(['cp', filename, drive_path])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMLD1-5iOhO",
        "colab_type": "code",
        "outputId": "11740cf7-ac8a-40e0-d541-edc73454b2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "load_or_copy('newsqa.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists in drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEaDd-xeifBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv newsqa.tar.gz newsqa/maluuba/newsqa/newsqa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZQIuS1z5am",
        "colab_type": "text"
      },
      "source": [
        "Now we also download the CNN stories we will use to train the model. You can download the stories [here](https://cs.nyu.edu/~kcho/DMQA/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEu_ZT0GjfpE",
        "colab_type": "code",
        "outputId": "625c3b7c-15de-4c3d-d962-90471d672fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "load_or_copy('cnn_stories.tgz')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists in drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U-0iyWOkrbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv cnn_stories.tgz newsqa/maluuba/newsqa/cnn_stories.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59dHsLC39-b",
        "colab_type": "text"
      },
      "source": [
        "Now we upload the Java dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGAKCBMEa_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImAW_vR84MBt",
        "colab_type": "code",
        "outputId": "1d38d3fd-6890-4836-b93a-8e923fc9fcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "load_or_copy('stanford-postagger-2015-12-09.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists in drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G-nUqCN-wP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r stanford-postagger-2015-12-09.zip newsqa/maluuba/newsqa/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Cylm1voVWf",
        "colab_type": "text"
      },
      "source": [
        "Now we run the data processing script in the repository. We run the repository's tests to make sure the data was processed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RGUzWN0td9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd newsqa && python2 maluuba/newsqa/data_generator.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx-gzrvN1kyt",
        "colab_type": "code",
        "outputId": "d8f808b4-4e03-4ee1-e928-8b8496cdeb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!cd newsqa && python2 -m unittest discover ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 2020-05-04 00:25:51,936 - data_processing.py::__init__\n",
            "Loading dataset from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2020-05-04 00:25:51,936 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2020-05-04 00:25:52,456 - data_processing.py::__init__\n",
            "Loading stories from `/content/newsqa/maluuba/newsqa/cnn_stories.tgz`...\n",
            "Getting story texts: 100% 12.7k/12.7k [00:12<00:00, 1.05k stories/s] \n",
            "Setting story texts: 100% 120k/120k [00:03<00:00, 37.0k questions/s] \n",
            "[INFO] 2020-05-04 00:26:07,792 - data_processing.py::__init__\n",
            "Done loading dataset.\n",
            "Checking for possible corruption: 100% 120k/120k [00:01<00:00, 103k questions/s]\n",
            ".[INFO] 2020-05-04 00:26:09,045 - data_processing.py::dump\n",
            "Packaging dataset to `/content/newsqa/combined-newsqa-data-v1.json`.\n",
            "Building json: 100% 120k/120k [00:05<00:00, 21.7k questions/s] \n",
            "Checking for possible corruption: 100% 12.7k/12.7k [00:00<00:00, 18.7k stories/s]\n",
            "Gathering answers: 100% 120k/120k [00:01<00:00, 107k questions/s]\n",
            "..[INFO] 2020-05-04 00:26:27,733 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/combined-newsqa-data-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 58.6k questions/s]\n",
            "Comparing stories: 100% 120k/120k [00:30<00:00, 3.93k rows/s]\n",
            ".[INFO] 2020-05-04 00:27:03,871 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-tokenized-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 58.3k questions/s]\n",
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 12 tests in 78.046s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqIW19KgZLZ-",
        "colab_type": "text"
      },
      "source": [
        "Now let's save the data to Drive. Once the code below runs, we will have a copy of the data that'll persist through different Colab kernel sessions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5vEtQw8Fp9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upload_to_drive(filepath):\n",
        "  \"\"\"Copy a file to Drive.\"\"\"\n",
        "  drive_path = base_dir + filepath.split('/')[-1]\n",
        "  subprocess.call(['cp', filepath, drive_path])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x__nql9IZeoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('newsqa/split_data/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfDRjztAZrnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('newsqa/split_data/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG3h7uLBZu6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('newsqa/split_data/dev.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHs9v9ltxWaw",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Download GloVe embeddings\n",
        "\n",
        "We need to download a set of pretrained word embeddings that were created using the Stanford GloVe (Global Vectors) algorithm, an unsupervised learning algorithm that uses a large set of words.\n",
        "\n",
        "You can run the code below to download the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSqzvR5HDI3s",
        "colab_type": "code",
        "outputId": "234bf9a4-617a-4d9a-9ac3-d22551093309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!mkdir -p glove && wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 02:55:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-07 02:56:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-07 02:56:00--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 26s  \n",
            "\n",
            "2020-05-07 03:02:26 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJGTszJqEF_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('glove.6B.zip', 'r') as z:\n",
        "    z.extractall('glove')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tLHS5rFF5T3",
        "colab_type": "text"
      },
      "source": [
        "I'll upload the 100-dimensional embeddings we'll be using for the model to Drive so I don't have to download the entire zip file again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A12RPPJ6Fo6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_to_drive('glove/glove.6B.100d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX926FhOGI1a",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to trim the embeddings down to only the relevant encodings for our training set are kept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhlqqcp4HYto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "MAX_DOCUMENT_LENGTH = 200\n",
        "\n",
        "\n",
        "def tokenize(words):\n",
        "  \"\"\"Transform a string of words into uniformly lower case tokens.\"\"\"\n",
        "  return [w.lower() for w in words.split(' ')]\n",
        "\n",
        "\n",
        "def load_data(csv_path):\n",
        "  \"\"\"Load the data from the CSV files generated by the Maluuba News QA repository.\"\"\"\n",
        "  stories = {}\n",
        "  with open(csv_path) as f:\n",
        "    for i, row in enumerate(csv.reader(f)):\n",
        "      if i == 0:  # Header row.\n",
        "        continue\n",
        "\n",
        "      doc_id = row[0]\n",
        "      existing_stories = stories.setdefault(doc_id, [])\n",
        "\n",
        "      doc_text = row[1]\n",
        "      if existing_stories and doc_text == existing_stories[0]['document_text']:\n",
        "        doc_words = existing_stories[0]['document_words']\n",
        "      else:\n",
        "        doc_words = tokenize(doc_text)[:MAX_DOCUMENT_LENGTH]\n",
        "\n",
        "      q_text = row[2]\n",
        "      q_words = tokenize(q_text)\n",
        "\n",
        "      answer = row[3]\n",
        "      ans_indices = []\n",
        "      for chunk in answer.split(','):\n",
        "        start, end = (int(idx) for idx in chunk.split(':'))\n",
        "        if end < MAX_DOCUMENT_LENGTH:\n",
        "          ans_indices.extend(range(start, end))\n",
        "      ans_text = ' '.join(doc_words[i] for i in ans_indices)\n",
        "\n",
        "      if ans_indices:\n",
        "        existing_stories.append({\n",
        "            'document_id': doc_id,\n",
        "            'document_text': doc_text,\n",
        "            'document_words': doc_words,\n",
        "            'answer_text': ans_text,\n",
        "            'answer_indices': ans_indices,\n",
        "            'question_text': q_text,\n",
        "            'question_words': q_words,\n",
        "        })\n",
        "  return stories\n",
        "\n",
        "\n",
        "def trim_embeddings():\n",
        "  \"\"\"Trim the Stanford GloVe embeddings to the 10,000 most common words in the training set.\"\"\"\n",
        "  doc_counts = Counter()\n",
        "  q_counts = Counter()\n",
        "\n",
        "  for data in [load_data(base_dir + 'train.csv').values(),\n",
        "               load_data(base_dir + 'test.csv').values()]:\n",
        "    for stories in data:\n",
        "      if stories:\n",
        "        doc_counts.update(stories[0]['document_words'])\n",
        "        for story in stories:\n",
        "          q_counts.update(story['question_words'])\n",
        "  \n",
        "  keep = set()\n",
        "  for word, _ in q_counts.most_common(5000):\n",
        "    keep.add(word)\n",
        "  for word, _ in doc_counts.most_common():\n",
        "    if len(keep) >= 10000:\n",
        "      break\n",
        "    keep.add(word)\n",
        "\n",
        "  with open(base_dir + 'glove.6B.100d.txt') as f:\n",
        "    with open(base_dir + 'glove.6B.100d.trimmed.txt', 'w') as f2:\n",
        "      for line in f:\n",
        "        if line.split(' ')[0] in keep:\n",
        "          f2.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vKKzo8QzM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trim_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1kZ_sCJ9fJ",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Data for the Model\n",
        "\n",
        "Below is code that will prepare the training and test sets for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N8uX2m1hZCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "END_WORD = '<END>'\n",
        "PADDING_WORD = '<PAD>'\n",
        "START_WORD = '<START>'\n",
        "UNKNOWN_WORD = '<UNK>'\n",
        "MAX_BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqzvh6iEhhMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_idx = {}\n",
        "idx_to_word = []\n",
        "\n",
        "def add_word(word):\n",
        "  \"\"\"Add a word to the dictionary.\"\"\"\n",
        "  if word in word_to_idx:\n",
        "    return word_to_idx[word]\n",
        "  idx = len(idx_to_word)\n",
        "  idx_to_word.append(word)\n",
        "  word_to_idx[word] = idx\n",
        "  return idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6zKrR5YiV3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "END_TOKEN = add_word(END_WORD)\n",
        "PADDING_TOKEN = add_word(PADDING_WORD)\n",
        "START_TOKEN = add_word(START_WORD)\n",
        "UNKNOWN_TOKEN = add_word(UNKNOWN_WORD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUtUjhLVg5Vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_path = base_dir + 'glove.6B.100d.trimmed.txt'\n",
        "\n",
        "with open(embeddings_path) as f:\n",
        "  line = f.readline()\n",
        "  chunks = line.split(' ')\n",
        "  dim = len(chunks) - 1\n",
        "  f.seek(0)\n",
        "\n",
        "  vocab_size = sum(1 for line in f)\n",
        "  vocab_size += 4\n",
        "  f.seek(0)\n",
        "\n",
        "  glove = np.ndarray((vocab_size, dim), dtype=np.float32)\n",
        "\n",
        "  for token in [START_TOKEN, END_TOKEN, UNKNOWN_TOKEN, PADDING_TOKEN]:\n",
        "    glove[token] = np.random.normal(0, 0.02, dim)\n",
        "\n",
        "  for line in f:\n",
        "    chunks = line.split(' ')\n",
        "    idx = add_word(chunks[0])\n",
        "    glove[idx] = [float(chunk) for chunk in chunks[1:]]\n",
        "    if len(word_to_idx) >= vocab_size:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siYBTQFVxsQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def look_up_word(word):\n",
        "  \"\"\"Look up token for word.\"\"\"\n",
        "  return word_to_idx.get(word, UNKNOWN_TOKEN)\n",
        "\n",
        "\n",
        "def look_up_token(idx):\n",
        "  \"\"\"Look up word for token.\"\"\"\n",
        "  return idx_to_word[idx]\n",
        "\n",
        "\n",
        "def prepare_batch(batch):\n",
        "  \"\"\"Prepare a batch of data for training.\"\"\"\n",
        "  ids_to_indices = {}\n",
        "  doc_ids = []\n",
        "  doc_text = []\n",
        "  doc_words = []\n",
        "  ans_text = []\n",
        "  ans_indices = []\n",
        "  q_text = []\n",
        "  q_input_words = []\n",
        "  q_output_words = []\n",
        "  for i, entry in enumerate(batch):\n",
        "    ids_to_indices.setdefault(entry['document_id'], []).append(i)\n",
        "    doc_ids.append(entry['document_id'])\n",
        "    doc_text.append(entry['document_text'])\n",
        "    doc_words.append(entry['document_words'])\n",
        "    ans_text.append(entry['answer_text'])\n",
        "    ans_indices.append(entry['answer_indices'])\n",
        "    q_text.append(entry['question_text'])\n",
        "    q_words = entry['question_words']\n",
        "    q_input_words.append([START_WORD] + q_words)\n",
        "    q_output_words.append(q_words + [END_WORD])\n",
        "\n",
        "  batch_size = len(batch)\n",
        "  max_doc_len = max((len(doc) for doc in doc_words), default=0)\n",
        "  max_ans_len = max((len(ans) for ans in ans_indices), default=0)\n",
        "  max_q_len = max((len(q) for q in q_input_words), default=0)\n",
        "\n",
        "  doc_tokens = np.zeros((batch_size, max_doc_len), dtype=np.int32)\n",
        "  doc_lens = np.zeros(batch_size, dtype=np.int32)\n",
        "  ans_labels = np.zeros((batch_size, max_doc_len), dtype=np.int32)\n",
        "  ans_masks = np.zeros((batch_size, max_ans_len, max_doc_len), dtype=np.int32)\n",
        "  ans_lens = np.zeros(batch_size, dtype=np.int32)\n",
        "  q_input_tokens = np.zeros((batch_size, max_q_len), dtype=np.int32)\n",
        "  q_output_tokens = np.zeros((batch_size, max_q_len), dtype=np.int32)\n",
        "  q_lens = np.zeros(batch_size, dtype=np.int32)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    for j, word in enumerate(doc_words[i]):\n",
        "      doc_tokens[i,j] = look_up_word(word)\n",
        "    doc_lens[i] = len(doc_words[i])\n",
        "\n",
        "    for j, idx in enumerate(ans_indices[i]):\n",
        "      for shared_i in ids_to_indices[batch[i]['document_id']]:\n",
        "        ans_labels[shared_i, idx] = 1\n",
        "      ans_masks[i, j, idx] = 1\n",
        "    ans_lens[i] = len(ans_indices[i])\n",
        "\n",
        "    for j, word in enumerate(q_input_words[i]):\n",
        "      q_input_tokens[i, j] = look_up_word(word)\n",
        "    for j, word in enumerate(q_output_words[i]):\n",
        "      q_output_tokens[i, j] = look_up_word(word)\n",
        "    q_lens[i] = len(q_input_words[i])\n",
        "\n",
        "  return {\n",
        "      'size': batch_size,\n",
        "      'document_ids': doc_ids,\n",
        "      'document_text': doc_text,\n",
        "      'document_words': doc_words,\n",
        "      'document_tokens': doc_tokens,\n",
        "      'document_lengths': doc_lens,\n",
        "      'answer_text': ans_text,\n",
        "      'answer_indices': ans_indices,\n",
        "      'answer_labels': ans_labels,\n",
        "      'answer_masks': ans_masks,\n",
        "      'question_text': q_text,\n",
        "      'question_input_tokens': q_input_tokens,\n",
        "      'question_output_tokens': q_output_tokens,\n",
        "      'question_lengths': q_lens,\n",
        "  }\n",
        "\n",
        "\n",
        "def process_stories(stories):\n",
        "  \"\"\"Process the stories from one of the input CSV files.\"\"\"\n",
        "  batch = []\n",
        "  vals = list(stories.values())\n",
        "  random.shuffle(vals)\n",
        "  for story in vals:\n",
        "    if len(batch) + len(story) > MAX_BATCH_SIZE:\n",
        "      yield prepare_batch(batch)\n",
        "      batch = []\n",
        "    batch.extend(story)\n",
        "  if batch:\n",
        "    yield prepare_batch(batch)\n",
        "\n",
        "\n",
        "def training_data():\n",
        "  \"\"\"Get the training data.\"\"\"\n",
        "  return process_stories(load_data(base_dir + 'train.csv'))\n",
        "\n",
        "\n",
        "def test_data():\n",
        "  \"\"\"Get the test data.\"\"\"\n",
        "  return process_stories(load_data(base_dir + 'test.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0gVn2_a0c9b",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "Below we will define the model which will generate questions and answers. We make use of Keras' `Bidirectional` cell. This has a recurrent layer store hidden state for processing sequences forwards and backwards.\n",
        "\n",
        "Below in the first cell is the part of the model which predicts if words in the story is a part of an answer or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G1Lp6oH0-4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, Dense\n",
        "\n",
        "VOCAB_SIZE = glove.shape[0]\n",
        "EMBEDDING_DIMS = glove.shape[1]\n",
        "\n",
        "GRU_UNITS = 100\n",
        "DOC_SIZE = None\n",
        "ANSWER_SIZE = None\n",
        "Q_SIZE = None\n",
        "\n",
        "document_tokens = Input(shape=(DOC_SIZE,), name='document_tokens')\n",
        "\n",
        "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIMS,\n",
        "                      weights=[glove], mask_zero=True, name='embedding')\n",
        "document_emb = embedding(document_tokens)\n",
        "\n",
        "answer_outputs = Bidirectional(GRU(GRU_UNITS, return_sequences=True),\n",
        "                               name='answer_outputs')(document_emb)\n",
        "answer_tags = Dense(2, activation='softmax', name='answer_tags')(answer_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl1S83C32wiL",
        "colab_type": "text"
      },
      "source": [
        "The second part of the model is a encoder-decoder network that takes an answer and tries to generate a matching question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNlHgUq827vX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "encoder_input_mask = Input(shape=(ANSWER_SIZE, DOC_SIZE),\n",
        "                           name='encoder_input_mask')\n",
        "encoder_inputs = Lambda(lambda x: K.batch_dot(x[0], x[1]),\n",
        "                        name='encoder_inputs')([encoder_input_mask,\n",
        "                                                answer_outputs])\n",
        "encoder_cell = GRU(2 * GRU_UNITS, name='encoder_cell')(encoder_inputs)\n",
        "\n",
        "decoder_inputs = Input(shape=(Q_SIZE,), name='decoder_inputs')\n",
        "decoder_emb = embedding(decoder_inputs)\n",
        "decoder_emb.trainable = False\n",
        "\n",
        "decoder_cell = GRU(2 * GRU_UNITS, return_sequences=True, name='decoder_cell')\n",
        "decoder_states = decoder_cell(decoder_emb, initial_state=[encoder_cell])\n",
        "\n",
        "decoder_projection = Dense(VOCAB_SIZE, name='decoder_projection',\n",
        "                           activation='softmax', use_bias=False)\n",
        "decoder_outputs = decoder_projection(decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lcLrsuM5Nkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "total_model = Model([document_tokens, decoder_inputs, encoder_input_mask],\n",
        "                    [answer_tags, decoder_outputs])\n",
        "answer_model = Model(document_tokens, [answer_tags])\n",
        "decoder_initial_state_model = Model([document_tokens, encoder_input_mask],\n",
        "                                    [encoder_cell])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghi5c8LH5oub",
        "colab_type": "code",
        "outputId": "adcbe9d2-8b46-4ce3-9d45-a5b14a353367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "total_model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "document_tokens (InputLayer)    [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 100)    998200      document_tokens[0][0]            \n",
            "                                                                 decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "answer_outputs (Bidirectional)  (None, None, 200)    121200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "encoder_input_mask (InputLayer) [(None, None, None)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_inputs (Lambda)         (None, None, 200)    0           encoder_input_mask[0][0]         \n",
            "                                                                 answer_outputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_cell (GRU)              (None, 200)          241200      encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_cell (GRU)              (None, None, 200)    181200      embedding[1][0]                  \n",
            "                                                                 encoder_cell[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "answer_tags (Dense)             (None, None, 2)      402         answer_outputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_projection (Dense)      (None, None, 9982)   1996400     decoder_cell[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 3,538,602\n",
            "Trainable params: 3,538,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW67i2gn501_",
        "colab_type": "text"
      },
      "source": [
        "### Inference\n",
        "\n",
        "We can test performance of the model in two ways: first we feed a new document to the model and it identifies which tokens in the document are parts of answers. We can use the `answer_model` above for this task. The second is we feed the model answers and it generates matching questions. For the second task, we'll have to define a separate inference model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFW87cU09jM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_inputs_dynamic = Input(shape=(1,), name='decoder_inputs_dynamic')\n",
        "decoder_emb_dynamic = embedding(decoder_inputs_dynamic)\n",
        "decoder_init_state_dynamic = Input(shape=(2 * GRU_UNITS,),\n",
        "                                   name='decoder_init_state_dynamic')\n",
        "decoder_states_dynamic = decoder_cell(decoder_emb_dynamic,\n",
        "                                      initial_state=[\n",
        "                                        decoder_init_state_dynamic])\n",
        "decoder_outputs_dynamic = decoder_projection(decoder_states_dynamic)\n",
        "\n",
        "question_model = Model([decoder_inputs_dynamic, decoder_init_state_dynamic],\n",
        "                       [decoder_outputs_dynamic, decoder_states_dynamic])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG-jurhjX_Al",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "First we compile the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veCubJUvYB_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(lr=0.001)\n",
        "total_model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy'],\n",
        "    loss_weights=[1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kenJLZ00Yi-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 2000\n",
        "initial_epoch = 0\n",
        "weights_dir = base_dir + 'weights/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnuWzRrvY346",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = training_data()\n",
        "X_test = test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHAhdx65YpKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(initial_epoch + 1, EPOCHS + 1):\n",
        "  print('Epoch: ', epoch)\n",
        "  for i, batch in enumerate(X_train):\n",
        "    test_batch = next(X_test, None)\n",
        "    if test_batch is None:\n",
        "      X_test = test_data()\n",
        "      test_batch = next(X_test, None)\n",
        "\n",
        "    train_loss = total_model.train_on_batch(\n",
        "        [batch['document_tokens'], batch['question_input_tokens'],\n",
        "         batch['answer_masks']],\n",
        "        [np.expand_dims(batch['answer_labels'], axis=-1),\n",
        "         np.expand_dims(batch['question_output_tokens'], axis=-1)])\n",
        "    \n",
        "    test_loss = total_model.test_on_batch(\n",
        "        [test_batch['document_tokens'], test_batch['question_input_tokens'],\n",
        "         test_batch['answer_masks']],\n",
        "        [np.expand_dims(test_batch['answer_labels'], axis=-1),\n",
        "         np.expand_dims(test_batch['question_output_tokens'], axis=-1)])\n",
        "    \n",
        "    print('Train loss: {} | Test loss: {}'.format(train_loss, test_loss))\n",
        "  total_model.save_weights(weights_dir + 'weights_{}.hdf5'.format(epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}