{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jN2KARhZhHs",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 5: Paint\n",
        "\n",
        "## CycleGAN\n",
        "\n",
        "_Style transfer_ is a machine learning task where a neural network learns to transform instances from one set of data to make it appear to have come from a second set of training instances and vice versa. For example, a style transfer model may transform photos of landscapes to look like they were painted in a particular artist's style.\n",
        "\n",
        "[_Cycle-consistent adverserial networks_](https://arxiv.org/pdf/1703.10593.pdf) (CycleGAN) is a type of neural network architecture which learns style transfer between two sets of data without the need for paired samples. This is in contrast to previous style transfer networks which required paired samples from either dataset, such as [_pix2pix_](https://arxiv.org/pdf/1611.07004.pdf).\n",
        "\n",
        "### Model Overview\n",
        "\n",
        "CycleGANs are composed of two generators and two discriminators. One generator, $G_{AB}$, takes samples from dataset $A$ and transforms it to look like a sample from dataset $B$, the other generator, $G_{BA}$, does the inverse transformation ($B$ to $A$). There are also two discriminators, the first, $D_{A}$, determines if an instance is from dataset $A$ or was transformed by $G_{BA}$ to look like it was sampled from dataset $A$. The other discriminator, $D_B$, determines if samples are from dataset $B$ or were transformed by $G_{AB}$.\n",
        "\n",
        "## The Generators\n",
        "\n",
        "Typically the generators are usually using the [_U-Net_](https://arxiv.org/pdf/1505.04597.pdf) or [_ResNet_](https://arxiv.org/pdf/1512.03385.pdf) (residual network) architecture.\n",
        "\n",
        "### U-Net\n",
        "\n",
        "The U-Net is similar to a VAE except that it contains _skip connections_ or connections between downsampling and upsampling layers of the same size. U-Net also uses a new type of layer, [instance normalization](https://arxiv.org/pdf/1607.08022.pdf), which works similarly to the batch normalization layer except it normalizes individual observations instead of a batch. Instance normalization does not compute a mean and variance like batch normalization so it does not require a `momentum` parameter.\n",
        "\n",
        "<img src=\"https://github.com/shaohua0116/Group-Normalization-Tensorflow/raw/master/figure/gn.png\">\n",
        "\n",
        "A visual demonstraction of the different types of normalization done on CNNs during training visualized in 3-dimensions [[ref]](https://arxiv.org/pdf/1803.08494.pdf). The $H,W$ axis represents the spatial dimensions of the input. The $C$ axis represents the channels (i.e. filters) of the layer, and $N$ represents the \"batch axis,\" i.e. it iterates over each instance in the training batch.\n",
        "\n",
        "For U-Net, we also omit the scaling and shift parameters in instance normalization, therefore the layer does not learn these parameters during training.\n",
        "\n",
        "### ResNet\n",
        "\n",
        "TODO\n",
        "\n",
        "### Training\n",
        "\n",
        "In a CycleGAN, we use 3 separate loss functions for the generators:\n",
        "\n",
        "- _Validity_, do the images lok like they were sampled from the target dataset?\n",
        "\n",
        "- _Reconstruction_, do images passed through both generators look like the original image?\n",
        "\n",
        "- _Identity_, if we apply generators to images from their own dataset, is the image unchanged?\n",
        "\n",
        "## The Discriminators\n",
        "\n",
        "The discriminators for CycleGAN's do not output a single number, rather they output an 8 by 8 single channel tensor. This is borrowed from _PatchGAN_, a GAN architecture which divides the image into patches and guesses if each patch is real or fake. This helps the discriminator determine if an image is real or fake based on the style rather than the content.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqR-lYTragbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxydcnkLpICo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umTkTIeaSJKB",
        "colab_type": "text"
      },
      "source": [
        "## Implementing a CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AefTJLVrSMTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import (Input, Conv2D, Activation, UpSampling2D,\n",
        "                                     Concatenate, LeakyReLU)\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def unet_downsample(layer_input, filters, kernel_size=4,\n",
        "                    weight_init='glorot_uniform'):\n",
        "  \"\"\"Build a downsampling layer of U-Net.\"\"\"\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=2,\n",
        "             padding='same', kernel_initializer=weight_init)(layer_input)\n",
        "  x = InstanceNormalization(axis=3, center=False, scale=False)(x)\n",
        "  return Activation('relu')(x)\n",
        "\n",
        "\n",
        "def unet_upsample(layer_input, skip_input, filters, kernel_size=4,\n",
        "                    weight_init='glorot_uniform'):\n",
        "  \"\"\"Build an upsampling layer for U-Net.\"\"\"\n",
        "  x = UpSampling2D()(layer_input)\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=1,\n",
        "             padding='same', kernel_initializer=weight_init)(x)\n",
        "  x = InstanceNormalization(axis=-1, center=False, scale=False)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  return Concatenate()([x, skip_input])\n",
        "\n",
        "\n",
        "def build_generator_unet(input_shape, n_filters, channels,\n",
        "                         weight_init):\n",
        "    \"\"\"Build a U-Net generator.\"\"\"\n",
        "    x = Input(shape=input_shape)\n",
        "    d1 = unet_downsample(x, n_filters, weight_init=weight_init)\n",
        "    d2 = unet_downsample(d1, n_filters << 1, weight_init=weight_init)\n",
        "    d3 = unet_downsample(d2, n_filters << 2, weight_init=weight_init)\n",
        "    d4 = unet_downsample(d3, n_filters << 3, weight_init=weight_init)\n",
        "    u1 = unet_upsample(d4, d3, n_filters << 2, weight_init=weight_init)\n",
        "    u2 = unet_upsample(u1, d2, n_filters << 1, weight_init=weight_init)\n",
        "    u3 = unet_upsample(u2, d1, n_filters, weight_init=weight_init)\n",
        "    u4 = UpSampling2D()(u3)\n",
        "    output = Conv2D(filters=channels, kernel_size=4, strides=1, padding='same',\n",
        "                    activation='tanh', kernel_initializer=weight_init)(u4)\n",
        "    return Model(x, output)\n",
        "\n",
        "\n",
        "def discriminator_conv_layer(layer_input, filters, strides=2,\n",
        "                             weight_init='glorot_uniform', normalize=True):\n",
        "  \"\"\"Build a 4x4 convolutional layer in the discriminator.\"\"\"\n",
        "  x = Conv2D(filters, kernel_size=4, strides=strides,\n",
        "             padding='same', kernel_initializer=weight_init)(layer_input)\n",
        "  if normalize:\n",
        "    x = InstanceNormalization(axis=-1, center=False, scale=False)(x)\n",
        "  return LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "\n",
        "def build_discriminator(input_shape, n_filters, channels,\n",
        "                        weight_init):\n",
        "  \"\"\"Build a discriminator for the CycleGAN.\"\"\"\n",
        "  model_input = Input(shape=input_shape)\n",
        "  x = discriminator_conv_layer(model_input, n_filters, strides=2,\n",
        "                               weight_init=weight_init, normalize=False)\n",
        "  x = discriminator_conv_layer(x, n_filters << 1, strides=2,\n",
        "                               weight_init=weight_init)\n",
        "  x = discriminator_conv_layer(x, n_filters << 2, strides=2,\n",
        "                               weight_init=weight_init)\n",
        "  x = discriminator_conv_layer(x, n_filters << 3, strides=2,\n",
        "                               weight_init=weight_init)\n",
        "  model_output = Conv2D(filters=1, kernel_size=4, strides=1, padding='same',\n",
        "                        kernel_initializer=weight_init)(x)\n",
        "  return Model(model_input, model_output)\n",
        "\n",
        "\n",
        "def set_trainable(model, value):\n",
        "  \"\"\"Set each layer of a model as trainable.\"\"\"\n",
        "  model.trainable = value\n",
        "  for l in model.layers:\n",
        "    l.trainable = value\n",
        "\n",
        "\n",
        "class CycleGAN(object):\n",
        "  \"\"\"Cycle-consistent adversarial network implementation.\"\"\"\n",
        "\n",
        "  def __init__(self, input_shape, generator_type, generator_n_filters,\n",
        "               discriminator_n_filters, learning_rate, lambda_validation,\n",
        "               lambda_reconstruction, lambda_identity):\n",
        "    # Build the generators.\n",
        "    weight_init = RandomNormal(mean=0.0, stddev=0.02)\n",
        "    if generator_type.lower() in ['unet', 'u-net']:\n",
        "      self.g_AB = build_generator_unet(input_shape, generator_n_filters,\n",
        "                                       input_shape[2], weight_init)\n",
        "      self.g_BA = build_generator_unet(input_shape, generator_n_filters,\n",
        "                                       input_shape[2], weight_init)\n",
        "    elif generator_type.lower() == 'resnet':\n",
        "      raise Exception('Not implemented yet')\n",
        "    else:\n",
        "      raise Exception('CycleGAN constructed with invalid generator type')\n",
        "    \n",
        "    # Build the discriminators.\n",
        "    self.d_A = build_discriminator(input_shape, discriminator_n_filters,\n",
        "                                   input_shape[2], weight_init)\n",
        "    self.d_B = build_discriminator(input_shape, discriminator_n_filters,\n",
        "                                   input_shape[2], weight_init)\n",
        "\n",
        "    # Compile the generators.\n",
        "    set_trainable(self.d_A, False)\n",
        "    set_trainable(self.d_B, False)\n",
        "\n",
        "    img_A = Input(shape=input_shape)\n",
        "    img_B = Input(shape=input_shape)\n",
        "\n",
        "    fake_B = self.g_AB(img_A)\n",
        "    fake_A = self.g_BA(img_B)\n",
        "\n",
        "    reconst_A = self.g_BA(fake_B)\n",
        "    reconst_B = self.g_AB(fake_A)\n",
        "\n",
        "    img_A_id = self.g_BA(img_A)\n",
        "    img_B_id = self.g_AB(img_B)\n",
        "\n",
        "    valid_A = self.d_A(fake_A)\n",
        "    valid_B = self.d_B(fake_B)\n",
        "\n",
        "    self.combined = Model(inputs=[img_A, img_B],\n",
        "                          outputs=[valid_A, valid_B, reconst_A, reconst_B,\n",
        "                                   img_A_id, img_B_id])\n",
        "    self.combined.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate, beta_1=0.5),\n",
        "        loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'],\n",
        "        loss_weights=[lambda_validation, lambda_validation,\n",
        "                      lambda_reconstruction, lambda_reconstruction,\n",
        "                      lambda_identity, lambda_identity])\n",
        "\n",
        "    set_trainable(self.d_A, True)\n",
        "    set_trainable(self.d_B, True)\n",
        "\n",
        "    # Compile the discriminators.\n",
        "    set_trainable(self.g_AB, False)\n",
        "    set_trainable(self.g_BA, False)\n",
        "\n",
        "    self.d_A.compile(optimizer=Adam(learning_rate=learning_rate, beta_1=0.5),\n",
        "                     loss=['mse'], metrics=['accuracy'])\n",
        "    self.d_B.compile(optimizer=Adam(learning_rate=learning_rate, beta_1=0.5),\n",
        "                     loss=['mse'], metrics=['accuracy'])\n",
        "\n",
        "    set_trainable(self.g_AB, True)\n",
        "    set_trainable(self.g_BA, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxA1nqbOafZy",
        "colab_type": "text"
      },
      "source": [
        "## Changing Pictures of Apples to Oranges (and Vice-Versa) with a CycleGAN\n",
        "\n",
        "### Downloading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5DpQHns2-D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_j7OGnl2-6u",
        "colab_type": "text"
      },
      "source": [
        "### Instantiating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3FIBlA_-L98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jDW1n7P-E8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a2o_cyclegan = CycleGAN(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                        generator_type='unet', generator_n_filters=32,\n",
        "                        discriminator_n_filters=32, learning_rate=0.0002,\n",
        "                        lambda_validation=1, lambda_reconstruction=10,\n",
        "                        lambda_identity=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJyf5oaIcaBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e768e35-6309-42c1-93ae-7936f1081f80"
      },
      "source": [
        "a2o_cyclegan.g_AB.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_36\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_52 (InputLayer)           [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 64, 64, 32)   1568        input_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_200 (Ins (None, 64, 64, 32)   0           conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 64, 64, 32)   0           instance_normalization_200[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 32, 32, 64)   32832       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_201 (Ins (None, 32, 32, 64)   0           conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 32, 32, 64)   0           instance_normalization_201[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 16, 16, 128)  131200      activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_202 (Ins (None, 16, 16, 128)  0           conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 16, 16, 128)  0           instance_normalization_202[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 8, 8, 256)    524544      activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_203 (Ins (None, 8, 8, 256)    0           conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 256)    0           instance_normalization_203[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_85 (UpSampling2D) (None, 16, 16, 256)  0           activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 16, 16, 128)  524416      up_sampling2d_85[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_204 (Ins (None, 16, 16, 128)  0           conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 16, 16, 128)  0           instance_normalization_204[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 16, 16, 256)  0           activation_165[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_86 (UpSampling2D) (None, 32, 32, 256)  0           concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 32, 32, 64)   262208      up_sampling2d_86[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_205 (Ins (None, 32, 32, 64)   0           conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 32, 32, 64)   0           instance_normalization_205[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 32, 32, 128)  0           activation_166[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_87 (UpSampling2D) (None, 64, 64, 128)  0           concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 64, 64, 32)   65568       up_sampling2d_87[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_206 (Ins (None, 64, 64, 32)   0           conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 64, 64, 32)   0           instance_normalization_206[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 64, 64, 64)   0           activation_167[0][0]             \n",
            "                                                                 activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_88 (UpSampling2D) (None, 128, 128, 64) 0           concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 128, 128, 3)  3075        up_sampling2d_88[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 1,545,411\n",
            "Trainable params: 1,545,411\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgUUrW_w2iD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "08a4e7a1-0714-4fbe-c9e3-dbfeb23bff19"
      },
      "source": [
        "a2o_cyclegan.d_A.summary()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_54 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_263 (Conv2D)          (None, 64, 64, 32)        1568      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_264 (Conv2D)          (None, 32, 32, 64)        32832     \n",
            "_________________________________________________________________\n",
            "instance_normalization_214 ( (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_265 (Conv2D)          (None, 16, 16, 128)       131200    \n",
            "_________________________________________________________________\n",
            "instance_normalization_215 ( (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_54 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_266 (Conv2D)          (None, 8, 8, 256)         524544    \n",
            "_________________________________________________________________\n",
            "instance_normalization_216 ( (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_55 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_267 (Conv2D)          (None, 8, 8, 1)           4097      \n",
            "=================================================================\n",
            "Total params: 694,241\n",
            "Trainable params: 694,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}