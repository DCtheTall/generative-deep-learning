{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WorldModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTDL03VEWIP",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 8: Play\n",
        "\n",
        "This notebook contains an implementation of training an agent for the [OpenAI Gym `CarRacing-v0` environment](https://gym.openai.com/envs/CarRacing-v0/).\n",
        "\n",
        "## World Model\n",
        "\n",
        "This notebook will be using the [World Model architecture](https://arxiv.org/abs/1803.10122) to train a model for the `CarRacing-v0` environment using the model's own generated \"dream\" of the environment. The code is based on the implementation in [this repository](https://github.com/AppliedDataSciencePartners/WorldModels).\n",
        "\n",
        "The model is broken up into 3 main components: a variational autoencoder (VAE), a recurrent neural network with a mixture density network (MDN-RNN), and finally a controller.\n",
        "\n",
        "### The Variational Autoencoder\n",
        "\n",
        "The VAE will be trained first to encode the observations of different game states into a into a normally distributed, lower-dimensional latent space.\n",
        "\n",
        "### The MDN-RNN\n",
        "\n",
        "The MDN-RNN is trained after the VAE. Its goal is to predict the distribution of the next possible state in the latent space and the future reward at that state using the VAE's encoding, the most recent action, and the current reward as input. It consists of an LSTM network and a mixture-density network (MDN) output layer allows the next state could be sampled from numerous different normal distributions.\n",
        "\n",
        "### The Controller\n",
        "\n",
        "The controller is a densely connected neural network whose input is the concatenation of the output of the VAE and the hidden state of the LSTM network. The network's 3 output neurons represent the 3 possible actions the agent can take (steer, accelerate, brake).\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V24D9sEgDUC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install Box2D gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPZ53O24EL77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT6C9TpPENws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl947KFAFHk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "base_dir = '/content/gdrive/My Drive/gdl_models/world/'\n",
        "rollout_dir = os.path.join(base_dir, 'rollout/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe0pn3RuLs84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(300, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZleHi4DWQ5",
        "colab_type": "text"
      },
      "source": [
        "## Generating the Rollout Data\n",
        "\n",
        "Below is code that will generate the _rollout data_, data made up of observations of an agent acting randomly in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8R1sJPRPL7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def scale_observation(obs):\n",
        "  \"\"\"Scale observation pixel values to [0, 1].\"\"\"\n",
        "  return obs.astype('float32') / 255.0\n",
        "\n",
        "\n",
        "def collect_rollout_data(total_episodes=200, timesteps=300,\n",
        "                         action_refresh_rate=20):\n",
        "  \"\"\"Collect the rollout data for training the VAE.\"\"\"\n",
        "  env = gym.make('CarRacing-v0')\n",
        "\n",
        "  for s in range(total_episodes):\n",
        "    print('Running episode:\\t', s)\n",
        "    episode_id = str(int(time.time()))\n",
        "    filename = os.path.join(rollout_dir, episode_id + '.npz')\n",
        "    obs = env.reset()\n",
        "    env.render()\n",
        "\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    done_sequence = []\n",
        "\n",
        "    reward = -0.1\n",
        "    done = False\n",
        "\n",
        "    for t in range(timesteps):\n",
        "      if t % action_refresh_rate == 0:\n",
        "        action = env.action_space.sample()\n",
        "      observations.append(scale_observation(obs))\n",
        "      actions.append(action)\n",
        "      rewards.append(reward)\n",
        "      done_sequence.append(done)\n",
        "\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      env.render()\n",
        "\n",
        "    np.savez_compressed(filename, obs=observations, action=actions,\n",
        "                        reward=rewards, done=done_sequence)\n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MIrjREPKc80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_rollout_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Q9g7HuPJy6",
        "colab_type": "text"
      },
      "source": [
        "## Implementing and Training the VAE\n",
        "\n",
        "Below we will implement the variational autoencoder (VAE) this model will use to encode the game state into a normal distribution in a lower-dimensional latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcmbpMw1P6sK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}