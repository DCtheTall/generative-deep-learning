{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WorldModel.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTDL03VEWIP",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 8: Play\n",
        "\n",
        "This notebook contains an implementation of training an agent for the [OpenAI Gym `CarRacing-v0` environment](https://gym.openai.com/envs/CarRacing-v0/).\n",
        "\n",
        "## World Model\n",
        "\n",
        "This notebook will be using the [World Model architecture](https://arxiv.org/abs/1803.10122) to train a model for the `CarRacing-v0` environment using the model's own generated \"dream\" of the environment. The code is based on the implementation in [this repository](https://github.com/AppliedDataSciencePartners/WorldModels).\n",
        "\n",
        "The model is broken up into 3 main components: a variational autoencoder (VAE), a recurrent neural network with a mixture density network (MDN-RNN), and finally a controller.\n",
        "\n",
        "### The Variational Autoencoder\n",
        "\n",
        "The VAE will be trained first to encode the observations of different game states into a into a normally distributed, lower-dimensional latent space.\n",
        "\n",
        "### The MDN-RNN\n",
        "\n",
        "The MDN-RNN is trained after the VAE. Its goal is to predict the distribution of the next possible state in the latent space and the future reward at that state using the VAE's encoding, the most recent action, and the current reward as input. It consists of an LSTM network and a mixture-density network (MDN) output layer allows the next state could be sampled from numerous different normal distributions.\n",
        "\n",
        "### The Controller\n",
        "\n",
        "The controller is a densely connected neural network whose input is the concatenation of the output of the VAE and the hidden state of the LSTM network. The network's 3 output neurons represent the 3 possible actions the agent can take (steer, accelerate, brake).\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V24D9sEgDUC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install Box2D gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPZ53O24EL77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT6C9TpPENws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl947KFAFHk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "base_dir = '/content/gdrive/My Drive/gdl_models/world/'\n",
        "rollout_dir = os.path.join(base_dir, 'rollout/')\n",
        "vae_weights_dir = os.path.join(base_dir, 'vae/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe0pn3RuLs84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(300, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPKbsSO5EdON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z_DIM = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZleHi4DWQ5",
        "colab_type": "text"
      },
      "source": [
        "## Generating the Rollout Data\n",
        "\n",
        "Below is code that will generate the _rollout data_, data made up of observations of an agent acting randomly in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8R1sJPRPL7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def scale_observation(obs):\n",
        "  \"\"\"Scale observation pixel values to [0, 1].\"\"\"\n",
        "  return obs.astype('float32') / 255.0\n",
        "\n",
        "\n",
        "def collect_rollout_data(total_episodes=1000, timesteps=300,\n",
        "                         action_refresh_rate=20):\n",
        "  \"\"\"Collect the rollout data for training the VAE.\"\"\"\n",
        "  env = gym.make('CarRacing-v0')\n",
        "\n",
        "  for s in range(total_episodes):\n",
        "    print('Running episode:\\t', s)\n",
        "    episode_id = str(int(time.time()))\n",
        "    filename = os.path.join(rollout_dir, episode_id + '.npz')\n",
        "    obs = env.reset()\n",
        "    env.render()\n",
        "\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    done_sequence = []\n",
        "\n",
        "    reward = -0.1\n",
        "    done = False\n",
        "\n",
        "    for t in range(timesteps):\n",
        "      if t % action_refresh_rate == 0:\n",
        "        action = env.action_space.sample()\n",
        "      observations.append(scale_observation(obs))\n",
        "      actions.append(action)\n",
        "      rewards.append(reward)\n",
        "      done_sequence.append(done)\n",
        "\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      env.render()\n",
        "\n",
        "    np.savez_compressed(filename, obs=observations, action=actions,\n",
        "                        reward=rewards, done=done_sequence)\n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MIrjREPKc80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_rollout_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Q9g7HuPJy6",
        "colab_type": "text"
      },
      "source": [
        "## Implementing and Training the VAE\n",
        "\n",
        "Below we will implement the variational autoencoder (VAE) this model will use to encode the game state into a normal distribution in a lower-dimensional latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcmbpMw1P6sK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cff9e75-fec6-4f00-9fb9-f06e2b1acd7c"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization,\n",
        "                                     LeakyReLU, Dropout, Flatten, Dense,\n",
        "                                     Lambda, Reshape, Conv2DTranspose,\n",
        "                                     Activation)\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sampling(args):\n",
        "  \"\"\"Sample an encoding from the learned distribution.\"\"\"\n",
        "  mu, log_var = args\n",
        "  return mu + K.exp(log_var / 2) * K.random_normal(shape=K.shape(mu))\n",
        "\n",
        "\n",
        "def step_decay_schedule(initial_lr, decay_factor=0.5, step_size=1):\n",
        "  \"\"\"Create a LearningRateScheduler callback to decay the learning rate during training.\"\"\"\n",
        "  def schedule(epoch):\n",
        "    return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "  return LearningRateScheduler(schedule)\n",
        "\n",
        "\n",
        "class VAE():\n",
        "  \"\"\"Implements a varational autoencoder (VAE) using Keras.\"\"\"\n",
        "\n",
        "  def __init__(self, input_shape, encoder_conv_filters,\n",
        "               encoder_conv_kernel_size, encoder_conv_strides,\n",
        "               encoder_activations, decoder_conv_filters,\n",
        "               decoder_conv_kernel_size, decoder_conv_strides,\n",
        "               decoder_activations, z_dim, use_batch_normalization=False,\n",
        "               use_dropout=False, dropout_rate=0.25):\n",
        "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "    x = encoder_input\n",
        "    for i in range(len(encoder_conv_kernel_size)):\n",
        "      x = Conv2D(filters=encoder_conv_filters[i],\n",
        "                 kernel_size=encoder_conv_kernel_size[i],\n",
        "                 strides=encoder_conv_strides[i], padding='same',\n",
        "                 name='encoder_conv_{}'.format(i + 1))(x)\n",
        "      if use_batch_normalization:\n",
        "        x = BatchNormalization()(x)\n",
        "      if encoder_activations[i] == 'lrelu':\n",
        "        x = LeakyReLU()(x)\n",
        "      else:\n",
        "        x = Activation(encoder_activations[i])(x)\n",
        "      if use_dropout:\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "    shape_before_flattening = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.z_dim = z_dim\n",
        "    self.mu = Dense(z_dim, name='mu')(x)\n",
        "    self.log_var = Dense(z_dim, name='log_var')(x)\n",
        "    self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
        "    encoder_output = Lambda(sampling,\n",
        "                            name='encoder_output')([self.mu, self.log_var])\n",
        "    self.encoder = Model(encoder_input, encoder_output)\n",
        "\n",
        "    decoder_input = Input(shape=(z_dim,), name='decoder_input')\n",
        "    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "    x = Reshape(shape_before_flattening)(x)\n",
        "    for i in range(len(decoder_conv_kernel_size)):\n",
        "      x = Conv2DTranspose(filters=decoder_conv_filters[i],\n",
        "                          kernel_size=decoder_conv_kernel_size[i],\n",
        "                          strides=decoder_conv_strides[i], padding='same',\n",
        "                          name='decoder_conv_t_{}'.format(i + 1))(x)\n",
        "      if i < len(decoder_conv_kernel_size) - 1:\n",
        "        if use_batch_normalization:\n",
        "          x = BatchNormalization()(x)\n",
        "      if decoder_activations[i] == 'lrelu':\n",
        "        x = LeakyReLU()(x)\n",
        "      else:\n",
        "        x = Activation(decoder_activations[i])(x)\n",
        "      if use_dropout and i < len(decoder_conv_kernel_size) - 1:\n",
        "          x = Dropout(rate=dropout_rate)(x)\n",
        "    decoder_output = x\n",
        "    self.decoder = Model(decoder_input, decoder_output)\n",
        "    self.model = Model(encoder_input, self.decoder(encoder_output))\n",
        "    self.compiled = False\n",
        "    self.learning_rate = None\n",
        "\n",
        "  def compile(self, learning_rate, r_loss_factor):\n",
        "    \"\"\"Compile the model.\"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    if self.compiled:\n",
        "      return\n",
        "    opt = Adam(lr=learning_rate)\n",
        "\n",
        "    def mse(y_act, y_pred):\n",
        "      return r_loss_factor * K.mean(K.square(y_act - y_pred), axis=(1, 2, 3))\n",
        "\n",
        "    def kl_divergence(y_act, y_pred):\n",
        "      return -0.5 * K.sum(\n",
        "        1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis=1)\n",
        "\n",
        "    def loss(y_act, y_pred):\n",
        "      return mse(y_act, y_pred) + kl_divergence(y_act, y_pred)\n",
        "    \n",
        "    self.model.compile(opt, loss=loss, metrics=[mse, kl_divergence],\n",
        "                       experimental_run_tf_function=False)\n",
        "    self.compiled = True\n",
        "\n",
        "  def fit_with_generator(self, data_flow, epochs, steps_per_epoch,\n",
        "                         checkpoint_path=None, lr_decay=1, initial_epoch=0,):\n",
        "    if not self.compiled:\n",
        "      raise Exception('Model not compiled')\n",
        "    if initial_epoch > 0:\n",
        "      self.load(checkpoint_path + 'weights_{:03d}.hdf5'.format(initial_epoch))\n",
        "    lr_sched = step_decay_schedule(initial_lr=self.learning_rate,\n",
        "                                   decay_factor=lr_decay, step_size=1)\n",
        "    callbacks = [lr_sched]\n",
        "    if checkpoint_path:\n",
        "      callbacks.append(ModelCheckpoint(\n",
        "          filepath=checkpoint_path + 'weights.hdf5', verbose=1,\n",
        "          save_weights_only=True))\n",
        "      callbacks.append(ModelCheckpoint(\n",
        "          filepath=checkpoint_path + 'weights_{epoch:03d}.hdf5', verbose=1,\n",
        "          save_weights_only=True))\n",
        "    self.model.fit_generator(data_flow, epochs=epochs, shuffle=True,\n",
        "                             callbacks=callbacks, initial_epoch=initial_epoch,\n",
        "                             steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8khl43JDjzjV",
        "colab_type": "text"
      },
      "source": [
        "I will initialize the model with mostly the same hyperparameters are the [original code](https://github.com/AppliedDataSciencePartners/WorldModels/blob/master/vae/arch.py), but with some modifications to see how they impact the performance of the overall model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhEw77JbjyRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "vae = VAE(input_shape=(96, 96, 3),\n",
        "          encoder_conv_filters=(32, 64, 64, 128),\n",
        "          encoder_conv_kernel_size=(4, 4, 4, 4),\n",
        "          encoder_conv_strides=(2, 2, 2, 2),\n",
        "          encoder_activations=('relu', 'relu', 'relu', 'relu'),\n",
        "          decoder_conv_filters=(64, 64, 32, 3),\n",
        "          decoder_conv_kernel_size=(5, 5, 6, 6),\n",
        "          decoder_conv_strides=(2, 2, 2, 2),\n",
        "          decoder_activations=('relu', 'relu', 'relu', 'sigmoid'),\n",
        "          z_dim=Z_DIM)\n",
        "vae.compile(LEARNING_RATE, r_loss_factor=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drpFtUCIRq9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "3de19463-f052-456b-91b6-7ea20418cc74"
      },
      "source": [
        "vae.model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 96, 96, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_1 (Conv2D)         (None, 48, 48, 32)   1568        encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 48, 48, 32)   0           encoder_conv_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_2 (Conv2D)         (None, 24, 24, 64)   32832       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 24, 24, 64)   0           encoder_conv_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_3 (Conv2D)         (None, 12, 12, 64)   65600       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 12, 12, 64)   0           encoder_conv_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_4 (Conv2D)         (None, 6, 6, 128)    131200      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 6, 6, 128)    0           encoder_conv_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 4608)         0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 32)           147488      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "log_var (Dense)                 (None, 32)           147488      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 32)           0           mu[0][0]                         \n",
            "                                                                 log_var[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 (None, 96, 96, 3)    536611      encoder_output[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,062,787\n",
            "Trainable params: 1,062,787\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMRGQIZ2F9ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "EPOCHS = 10\n",
        "N_IMGS = 300 * len(os.listdir(rollout_dir))\n",
        "STEPS_PER_EPOCH = N_IMGS // BATCH_SIZE\n",
        "N_LOADS_PER_BATCH = 300 // BATCH_SIZE\n",
        "IMAGE_SIZE = (96, 96)\n",
        "\n",
        "def vae_training_data():\n",
        "  \"\"\"Load the VAE training data.\"\"\"\n",
        "  fnames = os.listdir(rollout_dir)\n",
        "  fnames.sort()\n",
        "  while True:\n",
        "    for fname in fnames:\n",
        "      new_data = np.load(rollout_dir + fname)['obs']\n",
        "      data = np.zeros((BATCH_SIZE, *IMAGE_SIZE, 3))\n",
        "      for i in range(N_LOADS_PER_BATCH):\n",
        "        data[:,:,:,:] = new_data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE, :, :, :]\n",
        "        yield data, data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgZnfIQIHoRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = vae_training_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw4Ag6n5KUvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae.fit_with_generator(X_train, epochs=EPOCHS,\n",
        "                       steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                       checkpoint_path=vae_weights_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3JeL13ahjsN",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing the VAE\n",
        "\n",
        "First we will analyze how the VAE reconstructs images from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj8wXVk3hlIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae.model.load_weights(vae_weights_dir + 'weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmMd2mKhsBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = vae_training_data()\n",
        "next(X_train)\n",
        "next(X_train)\n",
        "batch, _ = next(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Ku7eWGh1Qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = batch[90]\n",
        "plt.imshow(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAgrIDrliDwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = vae.model.predict([[x]])[0]\n",
        "plt.imshow(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89fsx7HDjSAH",
        "colab_type": "text"
      },
      "source": [
        "Another way to test the performance of an autonecoder is to decode randomly sampled noise from the latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prszRgLFjJL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = plt.imshow(\n",
        "    vae.decoder.predict(np.random.normal(0.0, 1.0, size=(1, Z_DIM)))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}